{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPUを使うために必要\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        \n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \n",
    "        _, lstm_out = self.lstm(sentence)\n",
    "        tag_space = self.hidden2tag(lstm_out[0])\n",
    "        tag_scores = self.softmax(tag_space.squeeze())\n",
    "#         tag_scores = F.softmax(tag_space.squeeze(), dim = 1)\n",
    "        \n",
    "        \n",
    "        return tag_scores, lstm_out[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPUを使うために必要\n",
    "\n",
    "class LSTMClassifier_3NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        \n",
    "        super(LSTMClassifier_3NN, self).__init__()\n",
    "        hidden1_dim = hidden_dim\n",
    "        hidden2_dim = 256\n",
    "        hidden3_dim = 64\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden1_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.fc2 = nn.Linear(hidden2_dim, hidden3_dim)\n",
    "        self.fc3 = nn.Linear(hidden3_dim, tagset_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        \n",
    "        _, lstm_out = self.lstm(sentence)\n",
    "        x1 = F.relu(self.fc1(lstm_out[0]))\n",
    "        x2 = F.relu(self.fc2(x1))\n",
    "        x3 = self.fc3(x2)\n",
    "        tag_scores = self.softmax(x3.squeeze())\n",
    "#         tag_scores = F.softmax(tag_space.squeeze(), dim = 1)\n",
    "        \n",
    "        \n",
    "        return tag_scores, lstm_out[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPUを使うために必要\n",
    "\n",
    "class Simple_3NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        \n",
    "        super(Simple_3NN,self).__init__()\n",
    "        hidden1_dim = hidden_dim\n",
    "        hidden2_dim = 256\n",
    "        hidden3_dim = 64\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1 = nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.fc2 = nn.Linear(hidden2_dim, hidden3_dim)\n",
    "        self.fc3 = nn.Linear(hidden3_dim, tagset_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        x1 = F.relu(self.fc1(sentence))\n",
    "        x2 = F.relu(self.fc2(x1))\n",
    "        x3 = self.fc3(x2)\n",
    "#         return F.softmax(x3, dim = 1)\n",
    "        return F.log_softmax(x3, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前処理用の関数（形態素解析とか）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import re\n",
    "import neologdn\n",
    "\n",
    "\n",
    "\n",
    "# テキスト事前処理用の関数\n",
    "def clean_sentence(sentence):\n",
    "    tagger = MeCab.Tagger(\"-Owakati\")\n",
    "    # MeCabで分かち書き\n",
    "    sentence = tagger.parse(sentence)\n",
    "    # 全角半角統一、重ね表現除去\n",
    "    sentence = neologdn.normalize(sentence)\n",
    "    # アルファベットの大文字を小文字に変換\n",
    "    sentence = sentence.lower()\n",
    "    # 半角全角英数字除去\n",
    "    #sentence = re.sub(r'[０-９a-zA-Zａ-ｚＡ-Ｚ]+', \" \", sentence)\n",
    "    # 記号もろもろ除去\n",
    "    sentence = re.sub(r'[″ω\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、､。｡・･,\\./『』【】「」｢｣→←○《》≪≫\\n\\u3000]+', \"\", sentence)\n",
    "    # スペースで区切って形態素の配列へ\n",
    "    wakati = sentence.split(\" \")\n",
    "    # 空の要素は削除\n",
    "    wakati = list(filter((\"\").__ne__, wakati))\n",
    "    # sentenceとして返す\n",
    "    sentence = ''.join(wakati)\n",
    "    return sentence\n",
    "\n",
    "# 分かち書き用の関数\n",
    "def make_wakati(sentence):\n",
    "    tagger = MeCab.Tagger(\"-Owakati\")\n",
    "    # MeCabで分かち書き\n",
    "    sentence = tagger.parse(sentence)\n",
    "    # 記号もろもろ除去\n",
    "    sentence = re.sub('\\n', \"\", sentence)\n",
    "    # スペースで区切って形態素の配列へ\n",
    "    wakati = sentence.split(\" \")\n",
    "    # 空の要素は削除\n",
    "    wakati = list(filter((\"\").__ne__, wakati))\n",
    "    return wakati\n",
    "\n",
    "# 文章を単語IDの系列データに変換\n",
    "# PyTorchのLSTMのインプットになるデータなので、もちろんtensor型で\n",
    "def sentence2index_fast(sentence):\n",
    "    wakati = make_wakati(sentence)\n",
    "    return torch.tensor([model_fast.get_word_vector(w) for w in wakati], dtype=torch.long)\n",
    "\n",
    "def sentence2index(sentence):\n",
    "    wakati = make_wakati(sentence)\n",
    "    return torch.tensor([word2index[w] for w in wakati], dtype=torch.long)\n",
    "\n",
    "# データをバッチでまとめるための関数\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def train2batch(title, category, batch_size):\n",
    "    title_batch = []\n",
    "    category_batch = []\n",
    "    \n",
    "#     title_shuffle, category_shuffle = shuffle(title, category)\n",
    "    \n",
    "    for i in range(0, len(title), batch_size):\n",
    "        title_batch.append(title[i:i+batch_size])\n",
    "        category_batch.append(category[i:i+batch_size])\n",
    "        \n",
    "    return title_batch, category_batch\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_tokenizer(text):\n",
    "    soup = BeautifulSoup(text,\"lxml\")\n",
    "    clean_text = soup.get_text()\n",
    "    return [tok for tok in keitaiso_noun(clean_text)]\n",
    "\n",
    "\n",
    "\n",
    "# mecab\n",
    "def keitaiso_noun(text):\n",
    "    mecab = MeCab.Tagger('-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd')\n",
    "    mecab.parse('')\n",
    "    node = mecab.parseToNode(text)\n",
    "    word=[]\n",
    "    hinshi = [\"名詞\",\"動詞\",\"形容詞\",\"助動詞\",\"助詞\"]\n",
    "    while node:\n",
    "        feats = node.feature.split(',')\n",
    "        if feats[0] in hinshi :\n",
    "            try:\n",
    "                word.append(node.surface)  #単語を取得\n",
    "            except:\n",
    "                print(\"err: \" + str(node.surface))\n",
    "        node = node.next  #次の単語に進める\n",
    "    return word\n",
    "\n",
    "\n",
    "def blank_delete(student_list, student_answer_list):\n",
    "    return_student = []\n",
    "    return_answer = []\n",
    "    for id, text in enumerate(student_list):\n",
    "        if text != \"b\":\n",
    "            return_student.append(text)\n",
    "            return_answer.append(student_answer_list[id])\n",
    "            \n",
    "    return return_student, return_answer\n",
    "\n",
    "#datasetファイルから対応する問題の答えを取得\n",
    "def get_data(question_num):\n",
    "    import xlrd\n",
    "    student_list=[]\n",
    "    student_answer_list=[]\n",
    "    if(question_num==1):\n",
    "        book = xlrd.open_workbook('./dataset/result_j1.xlsx')\n",
    "    elif(question_num==2):\n",
    "        book = xlrd.open_workbook('./dataset/result_ss1.xlsx')\n",
    "    elif(question_num==3):\n",
    "        book = xlrd.open_workbook('./dataset/result_s1.xlsx')\n",
    "    elif(question_num==4):\n",
    "        book = xlrd.open_workbook('./dataset/result_s_sakuta.xlsx')\n",
    "    elif(question_num==5):\n",
    "        book = xlrd.open_workbook('./dataset/result_new_s.xlsx')            \n",
    "    else:\n",
    "        print(\"無効な問題番号です\")\n",
    "    sheet_1 = book.sheet_by_index(0)\n",
    "    #シートの2行目以降をリストに追加\n",
    "    for row in range(sheet_1.nrows):\n",
    "        if(row!=0):\n",
    "            student_list.append(sheet_1.cell(row, 1).value)\n",
    "            student_answer_list.append(int(sheet_1.cell(row,2).value))\n",
    "    \n",
    "    #アンサーが0→1 2→1　3,4→0とする(0が正解　1が不正解　△は不正解とする)     \n",
    "    for i in range(len(student_answer_list)):\n",
    "        if(student_answer_list[i]==0):\n",
    "            student_answer_list[i]=1\n",
    "        elif(student_answer_list[i]==2):\n",
    "            student_answer_list[i]=1\n",
    "        elif(student_answer_list[i]==3):\n",
    "            student_answer_list[i]=0\n",
    "        else:\n",
    "            student_answer_list[i]=0\n",
    "\n",
    "        \n",
    "        \n",
    "    return student_list,student_answer_list\n",
    "\n",
    "\n",
    "def self_cross_val(all_data, val_num):\n",
    "    import copy\n",
    "    \n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    data_num = len(all_data)\n",
    "    split_pos = []\n",
    "    base_data_num = int(data_num/val_num)\n",
    "    amari = data_num%val_num\n",
    "    pos = 0\n",
    "    for i in range(val_num):\n",
    "        if i != 0:\n",
    "            pos = split_pos[i-1]\n",
    "        if amari > 0:\n",
    "            split_pos.append(pos+base_data_num+1)\n",
    "            amari -= 1\n",
    "        else:\n",
    "            split_pos.append(pos+base_data_num)\n",
    "    print(split_pos)\n",
    "\n",
    "    before_pos = 0\n",
    "    for id, pos in enumerate(split_pos):\n",
    "        tmp_list = copy.copy(all_data)      \n",
    "        test_data.append(all_data[before_pos:pos])\n",
    "\n",
    "        del tmp_list[before_pos:pos]\n",
    "        train_data.append(tmp_list)\n",
    "    \n",
    "        before_pos = pos\n",
    "        \n",
    "    if val_num == 1:\n",
    "        train_data = copy.copy(test_data)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "        \n",
    "\n",
    "# init model\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if hasattr(m, 'weight') and (classname.find('Embedding') == -1):\n",
    "        nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        \n",
    "#文章を単語リストに変換する\n",
    "def analyzer(text):\n",
    "    mecab = MeCab.Tagger('-Owakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd')\n",
    "#     stop_words = ['i', 'a', 'an', 'the', 'to', 'and', 'or', 'if', 'is', 'are', 'am', 'it', 'this', 'that', 'of', 'from', 'in', 'on']\n",
    "    stop_words = ['']\n",
    "#     text = text.lower() # 小文字化\n",
    "    mecab.parse('')\n",
    "    text = mecab.parse(text)\n",
    "    text = text.replace('\\n', '') # 改行削除\n",
    "    text = text.replace('\\t', '') # タブ削除\n",
    "    text = re.sub(re.compile(r'[!-\\/:-@[-`{-~]'), ' ', text) # 記号をスペースに置き換え\n",
    "    text = text.split(' ') # スペースで区切る\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "#条件をつけて単語リストに追加する\n",
    "    for word in text:\n",
    "#         if (re.compile(r'^.*[0-9]+.*$').fullmatch(word) is not None): # 数字が含まれるものは除外\n",
    "#             continue\n",
    "        if word in stop_words: # ストップワードに含まれるものは除外\n",
    "            continue\n",
    "        if len(word) < 1: #0文字（空文字）は除外\n",
    "            continue\n",
    "        words.append(word)\n",
    "            \n",
    "    return words\n",
    "            \n",
    "\n",
    "def change_Tensor(input_data):\n",
    "    return_data = []\n",
    "    \n",
    "    for data in input_data:\n",
    "        data =  torch.Tensor(data)\n",
    "        return_data.append(data)\n",
    "        \n",
    "    return_data = torch.Tensor(return_data)\n",
    "    return return_data\n",
    "        \n",
    "\n",
    "\n",
    "def make_bert_vec(qnum, val_num):\n",
    "    \n",
    "    import pandas as pd\n",
    "    train_val_vec = []\n",
    "    test_val_vec = []\n",
    "    bert_vec = []\n",
    "\n",
    "    if qnum == 1:\n",
    "        all_df = pd.read_json('./dataset/output_bert_japanese.jsonl', orient='records', lines=True)\n",
    "    elif qnum == 2:\n",
    "        all_df = pd.read_json('./dataset/output_bert_social.jsonl', orient='records', lines=True)\n",
    "    elif qnum == 3 or qnum == 4:\n",
    "        all_df = pd.read_json('./dataset/output_bert_science.jsonl', orient='records', lines=True)\n",
    "    elif qnum == 5:\n",
    "        all_df = pd.read_json('./dataset/model/output_new_science.jsonl', orient='records', lines=True)\n",
    "        \n",
    "#         all_df = pd.read_json('./dataset/output_science.jsonl', orient='records', lines=True)\n",
    "#         all_df = pd.read_json('./dataset/output_science_1029.jsonl', orient='records', lines=True)\n",
    "        \n",
    "    all_features = all_df['features']\n",
    "    for l in all_features:\n",
    "        vecs = []\n",
    "        for m in l:\n",
    "            vec = ( ( m['layers'] )[0] )['values']\n",
    "            vecs.append(vec)\n",
    "        vecs = torch.Tensor(vecs)\n",
    "        bert_vec.append(vecs)\n",
    "    \n",
    "    return bert_vec\n",
    "\n",
    "def make_bert_vec_laboro(qnum, val_num, flag):\n",
    "    \n",
    "    import pandas as pd\n",
    "    train_val_vec = []\n",
    "    test_val_vec = []\n",
    "    bert_vec = []\n",
    "\n",
    "    \n",
    "    if flag == 1:\n",
    "        if qnum == 1:\n",
    "            all_df = pd.read_json('./dataset/model/output_japanese_base.jsonl', orient='records', lines=True)\n",
    "        elif qnum == 2:\n",
    "            all_df = pd.read_json('./dataset/model/output_social_base.jsonl', orient='records', lines=True)\n",
    "        elif qnum == 3 or qnum == 4:\n",
    "            all_df = pd.read_json('./dataset/model/output_science_base.jsonl', orient='records', lines=True)\n",
    "    elif flag == 2:\n",
    "        if qnum == 1:\n",
    "            all_df = pd.read_json('./dataset/model/output_japanese_large.jsonl', orient='records', lines=True)\n",
    "        elif qnum == 2:\n",
    "            all_df = pd.read_json('./dataset/model/output_social_large.jsonl', orient='records', lines=True)\n",
    "        elif qnum == 3 or qnum == 4:\n",
    "            all_df = pd.read_json('./dataset/model/output_science_large.jsonl', orient='records', lines=True)\n",
    "        elif qnum == 5:\n",
    "            all_df = pd.read_json('./dataset/model/new_science.jsonl', orient='records', lines=True)\n",
    "        \n",
    "\n",
    "        \n",
    "    all_features = all_df['features']\n",
    "    for l in all_features:\n",
    "        vecs = []\n",
    "        for m in l:\n",
    "            vec = ( ( m['layers'] )[0] )['values']\n",
    "            vecs.append(vec)\n",
    "        vecs = torch.Tensor(vecs)\n",
    "        bert_vec.append(vecs)\n",
    "    \n",
    "    return bert_vec\n",
    "\n",
    "\n",
    "def print_graph(train_loss, train_accu):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    %matplotlib inline\n",
    "\n",
    "    x = range(len(train_loss))\n",
    "    y = train_loss\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(x, y, label='train_loss_random')\n",
    "    ax.set_ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    x = range(len(train_accu))\n",
    "    y = train_accu\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(x, y, label='train_acc_random')\n",
    "    ax.set_ylabel('acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "#ランダムに誤採点を作成\n",
    "def make_change_answer(answer_list, diff_num):\n",
    "    re_ans_list = []\n",
    "    change_pos = []\n",
    "    change_num = random.sample(range(0, len(answer_list), 1), k=diff_num)\n",
    "    \n",
    "    print(change_num)\n",
    "    \n",
    "    for id, label in enumerate(answer_list):\n",
    "        if id in change_num:\n",
    "            change_pos.append(1)\n",
    "            if label == 0:\n",
    "                label = 1\n",
    "            else :\n",
    "                label = 0\n",
    "        else:\n",
    "            change_pos.append(0)\n",
    "        re_ans_list.append(label)\n",
    "        \n",
    "    return re_ans_list, change_pos\n",
    "\n",
    "def my_shuffle(seed, box):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(box)\n",
    "    \n",
    "    return box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "何問目について採点するかを決定します(1〜4) 1:国語, 2:社会, 3:理科, 4:理科変更後\n",
      ">>> 5\n",
      "使うモデルを選択してください 0:ランダム, 1:fasttext, 2:wiki, 3;BERT, 4:朝日新聞, 5:辞書モデル\n",
      ">>> 3\n",
      "使うモデルを選択してください 0: BERT京大 1:base, 2:large\n",
      ">>> 0\n",
      "2017\n",
      "2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "#データセット準備\n",
    "import MeCab\n",
    "\n",
    "print(\"何問目について採点するかを決定します(1〜4) 1:国語, 2:社会, 3:理科, 4:理科変更後\") \n",
    "qnum=int(input('>>> '))\n",
    "print(\"使うモデルを選択してください 0:ランダム, 1:fasttext, 2:wiki, 3;BERT, 4:朝日新聞, 5:辞書モデル\") \n",
    "model_select = int(input('>>> ')) \n",
    "print(\"使うモデルを選択してください 0: BERT京大 1:base, 2:large\") \n",
    "laboro_model = int(input('>>> ')) \n",
    "\n",
    "student_list,student_answer_list =get_data(qnum) #1:おばあちゃん #2:社会 #3:国語 ラベルは0が正解、1が不正解 \n",
    "print(len(student_list)) \n",
    "student_list, student_answer_list = blank_delete(student_list, student_answer_list) \n",
    "print(len(student_list)) \n",
    "student_count=len(student_list)\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "if model_select == 2:\n",
    "    model_wiki = KeyedVectors.load_word2vec_format('./model/fastText/model.vec', binary=False)\n",
    "elif model_select == 3:\n",
    "    from bert_juman import BertWithJumanModel\n",
    "    bert = BertWithJumanModel(\"./model/Japanese_L-12_H-768_A-12_E-30_BPE\")\n",
    "elif model_select ==4:\n",
    "    model_wiki = KeyedVectors.load_word2vec_format(\"./model/embeddings/cbow.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 回すよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok1\n",
      "[505, 1009, 1513, 2017]\n",
      "[505, 1009, 1513, 2017]\n",
      "[505, 1009, 1513, 2017]\n",
      "train_data: 1512 , train_label: 1512\n",
      "test_data: 505 , test_label: 505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:252: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier_3NN\n",
      "LSTM\n",
      "Linear\n",
      "Linear\n",
      "Linear\n",
      "LogSoftmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:302: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  time: 5.11  loss: 36.143646  acc: 0.52249\n",
      "epoch: 2  time: 5.08  loss: 34.742968  acc: 0.56878\n",
      "epoch: 3  time: 4.66  loss: 24.601427  acc: 0.78638\n",
      "epoch: 4  time: 4.73  loss: 20.739066  acc: 0.81878\n",
      "epoch: 5  time: 4.42  loss: 17.924646  acc: 0.86243\n",
      "epoch: 6  time: 4.94  loss: 17.683006  acc: 0.86772\n",
      "epoch: 7  time: 4.56  loss: 15.441466  acc: 0.87963\n",
      "epoch: 8  time: 4.44  loss: 14.466099  acc: 0.89815\n",
      "epoch: 9  time: 4.78  loss: 13.370382  acc: 0.90476\n",
      "epoch: 10  time: 4.78  loss: 11.148798  acc: 0.92460\n",
      "epoch: 11  time: 4.58  loss: 10.797220  acc: 0.92328\n",
      "epoch: 12  time: 4.89  loss: 9.938559  acc: 0.92923\n",
      "epoch: 13  time: 5.20  loss: 8.443794  acc: 0.94180\n",
      "epoch: 14  time: 4.85  loss: 8.880643  acc: 0.93386\n",
      "epoch: 15  time: 4.96  loss: 6.895373  acc: 0.95437\n",
      "epoch: 16  time: 4.65  loss: 6.271459  acc: 0.95833\n",
      "epoch: 17  time: 4.47  loss: 5.413398  acc: 0.96098\n",
      "epoch: 18  time: 5.19  loss: 6.429125  acc: 0.95635\n",
      "epoch: 19  time: 4.70  loss: 5.774955  acc: 0.95833\n",
      "epoch: 20  time: 4.60  loss: 5.815977  acc: 0.95899\n",
      "epoch: 21  time: 5.05  loss: 5.103059  acc: 0.96429\n",
      "epoch: 22  time: 5.10  loss: 4.729734  acc: 0.96561\n",
      "epoch: 23  time: 4.71  loss: 4.853711  acc: 0.96627\n",
      "epoch: 24  time: 4.67  loss: 4.909791  acc: 0.96759\n",
      "epoch: 25  time: 4.76  loss: 4.534337  acc: 0.97156\n",
      "epoch: 26  time: 5.01  loss: 3.203200  acc: 0.98016\n",
      "epoch: 27  time: 5.07  loss: 3.976211  acc: 0.97553\n",
      "epoch: 28  time: 4.84  loss: 2.729875  acc: 0.98148\n",
      "epoch: 29  time: 4.74  loss: 2.592089  acc: 0.98347\n",
      "epoch: 30  time: 5.03  loss: 3.919260  acc: 0.97884\n",
      "epoch: 31  time: 4.81  loss: 2.601484  acc: 0.98347\n",
      "epoch: 32  time: 4.68  loss: 2.503478  acc: 0.98214\n",
      "epoch: 33  time: 4.95  loss: 2.943403  acc: 0.98214\n",
      "epoch: 34  time: 5.14  loss: 5.352814  acc: 0.96032\n",
      "epoch: 35  time: 5.10  loss: 3.946988  acc: 0.97354\n",
      "epoch: 36  time: 4.50  loss: 2.580910  acc: 0.98214\n",
      "epoch: 37  time: 4.66  loss: 2.900581  acc: 0.98082\n",
      "epoch: 38  time: 4.62  loss: 2.908236  acc: 0.98148\n",
      "epoch: 39  time: 4.65  loss: 2.154206  acc: 0.98611\n",
      "epoch: 40  time: 5.35  loss: 2.350901  acc: 0.98347\n",
      "epoch: 41  time: 5.84  loss: 3.054842  acc: 0.98347\n",
      "epoch: 42  time: 4.50  loss: 3.636998  acc: 0.97553\n",
      "epoch: 43  time: 4.91  loss: 3.161196  acc: 0.97950\n",
      "epoch: 44  time: 6.57  loss: 2.187018  acc: 0.98743\n",
      "epoch: 45  time: 5.31  loss: 1.828319  acc: 0.98876\n",
      "epoch: 46  time: 4.89  loss: 1.598837  acc: 0.98810\n",
      "epoch: 47  time: 4.69  loss: 1.398300  acc: 0.98942\n",
      "epoch: 48  time: 4.58  loss: 1.275096  acc: 0.99008\n",
      "epoch: 49  time: 4.67  loss: 1.251063  acc: 0.99008\n",
      "epoch: 50  time: 4.83  loss: 1.254406  acc: 0.99008\n",
      "epoch: 51  time: 4.82  loss: 1.631692  acc: 0.99008\n",
      "epoch: 52  time: 4.68  loss: 2.219036  acc: 0.98611\n",
      "epoch: 53  time: 4.73  loss: 1.691257  acc: 0.98677\n",
      "epoch: 54  time: 4.48  loss: 2.040616  acc: 0.98942\n",
      "epoch: 55  time: 4.46  loss: 3.891156  acc: 0.97817\n",
      "epoch: 56  time: 4.65  loss: 3.674329  acc: 0.96825\n",
      "epoch: 57  time: 4.58  loss: 2.513022  acc: 0.98214\n",
      "epoch: 58  time: 4.79  loss: 2.578119  acc: 0.98016\n",
      "epoch: 59  time: 5.28  loss: 2.254645  acc: 0.98876\n",
      "epoch: 60  time: 5.70  loss: 1.588414  acc: 0.98876\n",
      "epoch: 61  time: 4.58  loss: 1.480145  acc: 0.99008\n",
      "epoch: 62  time: 4.72  loss: 1.155813  acc: 0.99140\n",
      "epoch: 63  time: 4.89  loss: 1.257911  acc: 0.99008\n",
      "epoch: 64  time: 4.55  loss: 1.168253  acc: 0.99074\n",
      "epoch: 65  time: 4.57  loss: 1.138387  acc: 0.98942\n",
      "epoch: 66  time: 4.88  loss: 1.018664  acc: 0.99008\n",
      "epoch: 67  time: 4.36  loss: 0.956510  acc: 0.99206\n",
      "epoch: 68  time: 4.81  loss: 0.981443  acc: 0.99074\n",
      "epoch: 69  time: 4.56  loss: 5.405760  acc: 0.96693\n",
      "epoch: 70  time: 4.55  loss: 3.229084  acc: 0.97817\n",
      "epoch: 71  time: 4.59  loss: 3.430189  acc: 0.97421\n",
      "epoch: 72  time: 4.47  loss: 3.597515  acc: 0.97685\n",
      "epoch: 73  time: 4.32  loss: 3.730002  acc: 0.97751\n",
      "epoch: 74  time: 4.71  loss: 2.161140  acc: 0.98479\n",
      "epoch: 75  time: 4.48  loss: 3.800957  acc: 0.97487\n",
      "epoch: 76  time: 4.45  loss: 2.484567  acc: 0.98214\n",
      "epoch: 77  time: 4.28  loss: 3.276048  acc: 0.98280\n",
      "epoch: 78  time: 4.56  loss: 1.919676  acc: 0.98810\n",
      "epoch: 79  time: 4.59  loss: 1.737111  acc: 0.98810\n",
      "epoch: 80  time: 4.44  loss: 1.263674  acc: 0.98942\n",
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:353: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_acc :  0.8910891089108911\n",
      "train_data: 1513 , train_label: 1513\n",
      "test_data: 504 , test_label: 504\n",
      "LSTMClassifier_3NN\n",
      "LSTM\n",
      "Linear\n",
      "Linear\n",
      "Linear\n",
      "LogSoftmax\n",
      "epoch: 1  time: 4.45  loss: 35.640593  acc: 0.53007\n",
      "epoch: 2  time: 4.26  loss: 29.074548  acc: 0.70588\n",
      "epoch: 3  time: 4.46  loss: 22.027417  acc: 0.82683\n",
      "epoch: 4  time: 4.45  loss: 19.783756  acc: 0.85129\n",
      "epoch: 5  time: 4.50  loss: 21.694061  acc: 0.83543\n",
      "epoch: 6  time: 4.38  loss: 19.949368  acc: 0.85988\n",
      "epoch: 7  time: 4.36  loss: 16.445970  acc: 0.88500\n",
      "epoch: 8  time: 4.33  loss: 14.582365  acc: 0.90218\n",
      "epoch: 9  time: 4.60  loss: 12.613007  acc: 0.92003\n",
      "epoch: 10  time: 4.52  loss: 11.734562  acc: 0.92664\n",
      "epoch: 11  time: 4.46  loss: 11.651910  acc: 0.92333\n",
      "epoch: 12  time: 4.35  loss: 10.635979  acc: 0.93655\n",
      "epoch: 13  time: 4.34  loss: 10.393773  acc: 0.93457\n",
      "epoch: 14  time: 4.46  loss: 10.142192  acc: 0.93258\n",
      "epoch: 15  time: 4.47  loss: 9.038360  acc: 0.93919\n",
      "epoch: 16  time: 4.34  loss: 7.742456  acc: 0.95572\n",
      "epoch: 17  time: 4.30  loss: 6.925877  acc: 0.96100\n",
      "epoch: 18  time: 4.59  loss: 7.258522  acc: 0.95440\n",
      "epoch: 19  time: 4.45  loss: 8.645626  acc: 0.94779\n",
      "epoch: 20  time: 4.62  loss: 7.242372  acc: 0.96100\n",
      "epoch: 21  time: 4.61  loss: 7.831321  acc: 0.95770\n",
      "epoch: 22  time: 4.66  loss: 8.236912  acc: 0.95109\n",
      "epoch: 23  time: 4.39  loss: 6.953853  acc: 0.96100\n",
      "epoch: 24  time: 4.80  loss: 6.178874  acc: 0.96960\n",
      "epoch: 25  time: 4.74  loss: 5.541881  acc: 0.97290\n",
      "epoch: 26  time: 4.60  loss: 5.637324  acc: 0.97026\n",
      "epoch: 27  time: 4.67  loss: 5.148084  acc: 0.97488\n",
      "epoch: 28  time: 4.54  loss: 5.219155  acc: 0.97026\n",
      "epoch: 29  time: 4.43  loss: 5.611251  acc: 0.97158\n",
      "epoch: 30  time: 4.40  loss: 4.561198  acc: 0.97621\n",
      "epoch: 31  time: 4.42  loss: 4.295215  acc: 0.97885\n",
      "epoch: 32  time: 4.45  loss: 4.179313  acc: 0.97819\n",
      "epoch: 33  time: 4.55  loss: 4.721572  acc: 0.97488\n",
      "epoch: 34  time: 4.47  loss: 5.211788  acc: 0.96960\n",
      "epoch: 35  time: 4.64  loss: 5.135225  acc: 0.96894\n",
      "epoch: 36  time: 4.55  loss: 5.879091  acc: 0.96629\n",
      "epoch: 37  time: 4.78  loss: 4.189954  acc: 0.97422\n",
      "epoch: 38  time: 4.48  loss: 3.569729  acc: 0.97885\n",
      "epoch: 39  time: 4.53  loss: 3.244799  acc: 0.98282\n",
      "epoch: 40  time: 4.28  loss: 4.665593  acc: 0.97092\n",
      "epoch: 41  time: 4.52  loss: 4.066610  acc: 0.97621\n",
      "epoch: 42  time: 4.55  loss: 3.374315  acc: 0.97951\n",
      "epoch: 43  time: 4.47  loss: 2.520397  acc: 0.98083\n",
      "epoch: 44  time: 4.30  loss: 1.976978  acc: 0.98678\n",
      "epoch: 45  time: 4.46  loss: 2.047930  acc: 0.98876\n",
      "epoch: 46  time: 5.22  loss: 1.886216  acc: 0.99075\n",
      "epoch: 47  time: 4.50  loss: 2.977890  acc: 0.98282\n",
      "epoch: 48  time: 4.54  loss: 2.593122  acc: 0.98282\n",
      "epoch: 49  time: 4.55  loss: 1.637905  acc: 0.99009\n",
      "epoch: 50  time: 4.37  loss: 2.837013  acc: 0.98480\n",
      "epoch: 51  time: 4.11  loss: 7.365191  acc: 0.95440\n",
      "epoch: 52  time: 4.47  loss: 4.318954  acc: 0.96761\n",
      "epoch: 53  time: 4.45  loss: 2.354758  acc: 0.98414\n",
      "epoch: 54  time: 4.66  loss: 1.978787  acc: 0.98744\n",
      "epoch: 55  time: 4.37  loss: 2.331475  acc: 0.98546\n",
      "epoch: 56  time: 4.44  loss: 2.556343  acc: 0.98480\n",
      "epoch: 57  time: 4.24  loss: 3.203876  acc: 0.97555\n",
      "epoch: 58  time: 4.51  loss: 2.798504  acc: 0.98149\n",
      "epoch: 59  time: 4.47  loss: 2.999363  acc: 0.97951\n",
      "epoch: 60  time: 4.44  loss: 7.646912  acc: 0.95373\n",
      "epoch: 61  time: 4.55  loss: 4.520326  acc: 0.97555\n",
      "epoch: 62  time: 4.27  loss: 2.147819  acc: 0.98810\n",
      "epoch: 63  time: 4.38  loss: 1.540154  acc: 0.99141\n",
      "epoch: 64  time: 4.38  loss: 1.333213  acc: 0.99207\n",
      "epoch: 65  time: 4.63  loss: 1.110300  acc: 0.99405\n",
      "epoch: 66  time: 4.53  loss: 1.516897  acc: 0.99207\n",
      "epoch: 67  time: 4.26  loss: 2.820750  acc: 0.98215\n",
      "epoch: 68  time: 4.35  loss: 4.622526  acc: 0.96827\n",
      "epoch: 69  time: 4.53  loss: 2.960001  acc: 0.98017\n",
      "epoch: 70  time: 4.94  loss: 1.469589  acc: 0.99009\n",
      "epoch: 71  time: 5.05  loss: 1.266703  acc: 0.99339\n",
      "epoch: 72  time: 4.26  loss: 1.887545  acc: 0.99009\n",
      "epoch: 73  time: 4.55  loss: 1.406540  acc: 0.99405\n",
      "epoch: 74  time: 4.53  loss: 1.347224  acc: 0.99339\n",
      "epoch: 75  time: 4.21  loss: 1.049085  acc: 0.99471\n",
      "epoch: 76  time: 4.45  loss: 0.991329  acc: 0.99537\n",
      "epoch: 77  time: 4.52  loss: 0.966321  acc: 0.99537\n",
      "epoch: 78  time: 5.29  loss: 1.031920  acc: 0.99405\n",
      "epoch: 79  time: 5.45  loss: 0.951658  acc: 0.99537\n",
      "epoch: 80  time: 4.44  loss: 0.949389  acc: 0.99471\n",
      "done.\n",
      "predict_acc :  0.8214285714285714\n",
      "train_data: 1513 , train_label: 1513\n",
      "test_data: 504 , test_label: 504\n",
      "LSTMClassifier_3NN\n",
      "LSTM\n",
      "Linear\n",
      "Linear\n",
      "Linear\n",
      "LogSoftmax\n",
      "epoch: 1  time: 4.36  loss: 35.513709  acc: 0.55519\n",
      "epoch: 2  time: 4.60  loss: 31.818561  acc: 0.62591\n",
      "epoch: 3  time: 4.98  loss: 24.377495  acc: 0.79379\n",
      "epoch: 4  time: 4.69  loss: 24.534613  acc: 0.81692\n",
      "epoch: 5  time: 4.63  loss: 20.371533  acc: 0.84336\n",
      "epoch: 6  time: 4.36  loss: 19.421482  acc: 0.85790\n",
      "epoch: 7  time: 5.37  loss: 17.554815  acc: 0.86649\n",
      "epoch: 8  time: 5.07  loss: 14.305780  acc: 0.89954\n",
      "epoch: 9  time: 4.59  loss: 13.098844  acc: 0.90747\n",
      "epoch: 10  time: 4.77  loss: 13.156487  acc: 0.90615\n",
      "epoch: 11  time: 4.82  loss: 11.508805  acc: 0.92862\n",
      "epoch: 12  time: 4.49  loss: 10.012118  acc: 0.93853\n",
      "epoch: 13  time: 4.82  loss: 9.510363  acc: 0.94184\n",
      "epoch: 14  time: 4.95  loss: 9.675847  acc: 0.94779\n",
      "epoch: 15  time: 5.69  loss: 13.206328  acc: 0.90681\n",
      "epoch: 16  time: 5.00  loss: 9.650683  acc: 0.93655\n",
      "epoch: 17  time: 4.95  loss: 8.283168  acc: 0.94977\n",
      "epoch: 18  time: 4.41  loss: 8.321746  acc: 0.94712\n",
      "epoch: 19  time: 4.73  loss: 6.640947  acc: 0.95902\n",
      "epoch: 20  time: 4.74  loss: 5.873101  acc: 0.96695\n",
      "epoch: 21  time: 4.36  loss: 6.000512  acc: 0.96365\n",
      "epoch: 22  time: 4.17  loss: 5.572593  acc: 0.96960\n",
      "epoch: 23  time: 4.35  loss: 5.177972  acc: 0.97092\n",
      "epoch: 24  time: 4.50  loss: 6.283984  acc: 0.95638\n",
      "epoch: 25  time: 4.95  loss: 6.058536  acc: 0.96233\n",
      "epoch: 26  time: 4.75  loss: 5.895905  acc: 0.96167\n",
      "epoch: 27  time: 4.70  loss: 5.610920  acc: 0.96761\n",
      "epoch: 28  time: 4.80  loss: 5.911988  acc: 0.96233\n",
      "epoch: 29  time: 4.98  loss: 5.677958  acc: 0.96695\n",
      "epoch: 30  time: 4.89  loss: 5.381696  acc: 0.96761\n",
      "epoch: 31  time: 4.73  loss: 4.001859  acc: 0.98083\n",
      "epoch: 32  time: 5.21  loss: 3.104639  acc: 0.98612\n",
      "epoch: 33  time: 5.07  loss: 2.664058  acc: 0.98876\n",
      "epoch: 34  time: 4.90  loss: 2.587830  acc: 0.98678\n",
      "epoch: 35  time: 5.07  loss: 2.785313  acc: 0.98348\n",
      "epoch: 36  time: 4.85  loss: 2.578683  acc: 0.98744\n",
      "epoch: 37  time: 4.64  loss: 2.524695  acc: 0.98678\n",
      "epoch: 38  time: 4.87  loss: 2.235083  acc: 0.98612\n",
      "epoch: 39  time: 5.03  loss: 3.264338  acc: 0.97885\n",
      "epoch: 40  time: 4.86  loss: 3.089588  acc: 0.98414\n",
      "epoch: 41  time: 4.80  loss: 3.246976  acc: 0.98149\n",
      "epoch: 42  time: 4.76  loss: 3.617001  acc: 0.98149\n",
      "epoch: 43  time: 5.29  loss: 2.513677  acc: 0.98480\n",
      "epoch: 44  time: 4.79  loss: 3.972003  acc: 0.97290\n",
      "epoch: 45  time: 4.76  loss: 3.752363  acc: 0.97224\n",
      "epoch: 46  time: 4.46  loss: 2.697587  acc: 0.98215\n",
      "epoch: 47  time: 4.42  loss: 4.892959  acc: 0.96431\n",
      "epoch: 48  time: 4.61  loss: 3.191758  acc: 0.97819\n",
      "epoch: 49  time: 4.72  loss: 2.250269  acc: 0.98149\n",
      "epoch: 50  time: 4.85  loss: 2.202484  acc: 0.98546\n",
      "epoch: 51  time: 4.53  loss: 2.154337  acc: 0.98215\n",
      "epoch: 52  time: 4.59  loss: 1.985687  acc: 0.99009\n",
      "epoch: 53  time: 4.75  loss: 1.794246  acc: 0.98612\n",
      "epoch: 54  time: 4.38  loss: 1.460448  acc: 0.99207\n",
      "epoch: 55  time: 5.02  loss: 1.373580  acc: 0.99141\n",
      "epoch: 56  time: 4.91  loss: 1.211515  acc: 0.99207\n",
      "epoch: 57  time: 5.20  loss: 1.152979  acc: 0.99273\n",
      "epoch: 58  time: 5.12  loss: 1.039661  acc: 0.99273\n",
      "epoch: 59  time: 4.86  loss: 1.102364  acc: 0.99207\n",
      "epoch: 60  time: 4.52  loss: 1.191275  acc: 0.99273\n",
      "epoch: 61  time: 4.75  loss: 2.315073  acc: 0.98612\n",
      "epoch: 62  time: 4.57  loss: 4.525653  acc: 0.97224\n",
      "epoch: 63  time: 4.50  loss: 4.611030  acc: 0.96629\n",
      "epoch: 64  time: 4.25  loss: 3.918753  acc: 0.97224\n",
      "epoch: 65  time: 4.46  loss: 2.826192  acc: 0.98414\n",
      "epoch: 66  time: 4.79  loss: 1.771794  acc: 0.98612\n",
      "epoch: 67  time: 4.75  loss: 1.335613  acc: 0.99207\n",
      "epoch: 68  time: 4.94  loss: 1.109503  acc: 0.99207\n",
      "epoch: 69  time: 5.06  loss: 1.112878  acc: 0.99141\n",
      "epoch: 70  time: 4.65  loss: 1.117237  acc: 0.99339\n",
      "epoch: 71  time: 5.33  loss: 1.161838  acc: 0.99207\n",
      "epoch: 72  time: 4.63  loss: 1.008405  acc: 0.99141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 73  time: 5.00  loss: 1.374908  acc: 0.98678\n",
      "epoch: 74  time: 4.79  loss: 4.599587  acc: 0.97753\n",
      "epoch: 75  time: 4.49  loss: 4.550635  acc: 0.97158\n",
      "epoch: 76  time: 5.04  loss: 2.881647  acc: 0.98612\n",
      "epoch: 77  time: 5.31  loss: 1.935341  acc: 0.98942\n",
      "epoch: 78  time: 4.81  loss: 1.526372  acc: 0.99273\n",
      "epoch: 79  time: 4.90  loss: 1.376803  acc: 0.99339\n",
      "epoch: 80  time: 5.25  loss: 1.312332  acc: 0.99339\n",
      "done.\n",
      "predict_acc :  0.8412698412698413\n",
      "train_data: 1513 , train_label: 1513\n",
      "test_data: 504 , test_label: 504\n",
      "LSTMClassifier_3NN\n",
      "LSTM\n",
      "Linear\n",
      "Linear\n",
      "Linear\n",
      "LogSoftmax\n",
      "epoch: 1  time: 4.68  loss: 35.298535  acc: 0.55254\n",
      "epoch: 2  time: 4.54  loss: 28.412291  acc: 0.70456\n",
      "epoch: 3  time: 4.57  loss: 19.130601  acc: 0.84534\n",
      "epoch: 4  time: 4.37  loss: 17.526681  acc: 0.86451\n",
      "epoch: 5  time: 4.35  loss: 14.350665  acc: 0.89954\n",
      "epoch: 6  time: 4.27  loss: 13.540524  acc: 0.90549\n",
      "epoch: 7  time: 4.71  loss: 13.438755  acc: 0.91143\n",
      "epoch: 8  time: 4.52  loss: 11.630311  acc: 0.92135\n",
      "epoch: 9  time: 4.27  loss: 10.004333  acc: 0.93589\n",
      "epoch: 10  time: 4.35  loss: 8.863965  acc: 0.94646\n",
      "epoch: 11  time: 4.39  loss: 11.080822  acc: 0.92069\n",
      "epoch: 12  time: 4.61  loss: 8.307463  acc: 0.95373\n",
      "epoch: 13  time: 5.09  loss: 6.676174  acc: 0.96167\n",
      "epoch: 14  time: 4.50  loss: 5.951420  acc: 0.96365\n",
      "epoch: 15  time: 4.61  loss: 5.728022  acc: 0.96629\n",
      "epoch: 16  time: 4.70  loss: 6.736690  acc: 0.95572\n",
      "epoch: 17  time: 4.35  loss: 6.447122  acc: 0.95902\n",
      "epoch: 18  time: 4.71  loss: 5.392985  acc: 0.96563\n",
      "epoch: 19  time: 4.76  loss: 4.992013  acc: 0.96894\n",
      "epoch: 20  time: 4.27  loss: 4.790729  acc: 0.97224\n",
      "epoch: 21  time: 4.47  loss: 3.690023  acc: 0.97819\n",
      "epoch: 22  time: 4.47  loss: 5.358985  acc: 0.96431\n",
      "epoch: 23  time: 4.43  loss: 6.495115  acc: 0.95770\n",
      "epoch: 24  time: 5.04  loss: 6.007923  acc: 0.96365\n",
      "epoch: 25  time: 5.70  loss: 4.034652  acc: 0.97753\n",
      "epoch: 26  time: 5.46  loss: 4.008550  acc: 0.97819\n",
      "epoch: 27  time: 5.50  loss: 3.974110  acc: 0.97290\n",
      "epoch: 28  time: 5.14  loss: 4.515750  acc: 0.97158\n",
      "epoch: 29  time: 6.24  loss: 5.537371  acc: 0.96233\n",
      "epoch: 30  time: 4.86  loss: 5.629567  acc: 0.96497\n",
      "epoch: 31  time: 5.44  loss: 3.888028  acc: 0.97687\n",
      "epoch: 32  time: 5.02  loss: 3.566689  acc: 0.98017\n",
      "epoch: 33  time: 5.07  loss: 3.330921  acc: 0.98017\n",
      "epoch: 34  time: 4.93  loss: 4.715888  acc: 0.96894\n",
      "epoch: 35  time: 4.90  loss: 3.576204  acc: 0.97753\n",
      "epoch: 36  time: 5.11  loss: 4.066309  acc: 0.97026\n",
      "epoch: 37  time: 5.66  loss: 2.659813  acc: 0.98414\n",
      "epoch: 38  time: 6.48  loss: 2.059608  acc: 0.98744\n",
      "epoch: 39  time: 5.09  loss: 2.055997  acc: 0.98876\n",
      "epoch: 40  time: 5.00  loss: 1.716667  acc: 0.99009\n",
      "epoch: 41  time: 4.89  loss: 1.680745  acc: 0.99009\n",
      "epoch: 42  time: 5.15  loss: 2.237726  acc: 0.98744\n",
      "epoch: 43  time: 4.85  loss: 1.897913  acc: 0.98942\n",
      "epoch: 44  time: 4.65  loss: 1.645278  acc: 0.99207\n",
      "epoch: 45  time: 4.82  loss: 1.535848  acc: 0.99141\n",
      "epoch: 46  time: 4.53  loss: 1.619597  acc: 0.99009\n",
      "epoch: 47  time: 5.29  loss: 3.666502  acc: 0.97621\n",
      "epoch: 48  time: 4.90  loss: 3.943705  acc: 0.97687\n",
      "epoch: 49  time: 5.33  loss: 3.349465  acc: 0.97951\n",
      "epoch: 50  time: 4.95  loss: 1.901451  acc: 0.98876\n",
      "epoch: 51  time: 5.58  loss: 2.149620  acc: 0.98744\n",
      "epoch: 52  time: 4.85  loss: 1.601598  acc: 0.99075\n",
      "epoch: 53  time: 5.03  loss: 1.618625  acc: 0.99141\n",
      "epoch: 54  time: 4.97  loss: 1.510432  acc: 0.99141\n",
      "epoch: 55  time: 4.95  loss: 1.495397  acc: 0.99207\n",
      "epoch: 56  time: 4.73  loss: 1.447088  acc: 0.99141\n",
      "epoch: 57  time: 4.60  loss: 1.524507  acc: 0.99141\n",
      "epoch: 58  time: 4.87  loss: 2.641369  acc: 0.98348\n",
      "epoch: 59  time: 4.67  loss: 4.331970  acc: 0.97753\n",
      "epoch: 60  time: 4.66  loss: 2.552479  acc: 0.98149\n",
      "epoch: 61  time: 5.16  loss: 1.959113  acc: 0.98348\n",
      "epoch: 62  time: 5.28  loss: 1.662271  acc: 0.98942\n",
      "epoch: 63  time: 4.83  loss: 1.556468  acc: 0.99075\n",
      "epoch: 64  time: 5.16  loss: 1.507281  acc: 0.99207\n",
      "epoch: 65  time: 5.42  loss: 1.436409  acc: 0.99207\n",
      "epoch: 66  time: 5.63  loss: 1.709600  acc: 0.98810\n",
      "epoch: 67  time: 4.85  loss: 1.955068  acc: 0.99075\n",
      "epoch: 68  time: 5.05  loss: 1.852361  acc: 0.98744\n",
      "epoch: 69  time: 4.46  loss: 1.309534  acc: 0.99075\n",
      "epoch: 70  time: 4.51  loss: 1.314731  acc: 0.99141\n",
      "epoch: 71  time: 4.66  loss: 1.378348  acc: 0.99207\n",
      "epoch: 72  time: 4.41  loss: 1.427992  acc: 0.99141\n",
      "epoch: 73  time: 4.38  loss: 1.447472  acc: 0.99141\n",
      "epoch: 74  time: 4.47  loss: 1.470839  acc: 0.99141\n",
      "epoch: 75  time: 4.60  loss: 1.370975  acc: 0.99207\n",
      "epoch: 76  time: 4.64  loss: 1.280118  acc: 0.99207\n",
      "epoch: 77  time: 4.85  loss: 1.337012  acc: 0.99207\n",
      "epoch: 78  time: 4.99  loss: 1.431159  acc: 0.99207\n",
      "epoch: 79  time: 4.46  loss: 1.423053  acc: 0.99207\n",
      "epoch: 80  time: 4.33  loss: 3.681036  acc: 0.97621\n",
      "done.\n",
      "predict_acc :  0.8591269841269841\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch : score\n",
      "0 : 0.8910891089108911\n",
      "1 : 0.8214285714285714\n",
      "2 : 0.8412698412698413\n",
      "3 : 0.8591269841269841\n",
      "[[0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1], []]\n",
      "predict_ave :  0.853228626434072\n",
      "[505, 1009, 1513, 2017]\n",
      "[505, 1009, 1513, 2017]\n",
      "[505, 1009, 1513, 2017]\n",
      "train_data: 1512 , train_label: 1512\n",
      "test_data: 505 , test_label: 505\n",
      "LSTMClassifier_3NN\n",
      "LSTM\n",
      "Linear\n",
      "Linear\n",
      "Linear\n",
      "LogSoftmax\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  time: 4.39  loss: 35.404145  acc: 0.50794\n",
      "epoch: 2  time: 5.12  loss: 32.385041  acc: 0.64616\n",
      "epoch: 3  time: 5.32  loss: 23.005494  acc: 0.81481\n",
      "epoch: 4  time: 5.03  loss: 19.325228  acc: 0.85979\n",
      "epoch: 5  time: 4.85  loss: 16.937448  acc: 0.88228\n",
      "epoch: 6  time: 4.97  loss: 16.332077  acc: 0.88426\n",
      "epoch: 7  time: 4.76  loss: 14.931267  acc: 0.89550\n",
      "epoch: 8  time: 6.02  loss: 14.166223  acc: 0.89749\n",
      "epoch: 9  time: 4.56  loss: 13.885885  acc: 0.90013\n",
      "epoch: 10  time: 4.99  loss: 12.540214  acc: 0.91534\n",
      "epoch: 11  time: 4.92  loss: 12.053243  acc: 0.91601\n",
      "epoch: 12  time: 5.08  loss: 11.584434  acc: 0.91997\n",
      "epoch: 13  time: 5.85  loss: 9.252439  acc: 0.93386\n",
      "epoch: 14  time: 5.28  loss: 8.361138  acc: 0.94378\n",
      "epoch: 15  time: 5.25  loss: 8.758567  acc: 0.94378\n",
      "epoch: 16  time: 5.29  loss: 7.131292  acc: 0.95701\n",
      "epoch: 17  time: 5.03  loss: 6.198308  acc: 0.96230\n",
      "epoch: 18  time: 5.44  loss: 6.024703  acc: 0.96164\n",
      "epoch: 19  time: 5.80  loss: 5.640687  acc: 0.96296\n",
      "epoch: 20  time: 4.45  loss: 5.789134  acc: 0.96495\n",
      "epoch: 21  time: 4.48  loss: 4.647106  acc: 0.97090\n",
      "epoch: 22  time: 4.75  loss: 5.882198  acc: 0.95899\n",
      "epoch: 23  time: 4.47  loss: 4.954982  acc: 0.97090\n",
      "epoch: 24  time: 5.98  loss: 4.390997  acc: 0.96892\n",
      "epoch: 25  time: 5.73  loss: 4.216778  acc: 0.97090\n",
      "epoch: 26  time: 4.96  loss: 4.509726  acc: 0.96892\n",
      "epoch: 27  time: 5.45  loss: 4.713412  acc: 0.96892\n",
      "epoch: 28  time: 5.36  loss: 3.196252  acc: 0.98082\n",
      "epoch: 29  time: 5.02  loss: 3.855860  acc: 0.97685\n",
      "epoch: 30  time: 4.68  loss: 3.550316  acc: 0.97685\n",
      "epoch: 31  time: 5.13  loss: 2.410374  acc: 0.98479\n",
      "epoch: 32  time: 4.53  loss: 2.083568  acc: 0.98810\n",
      "epoch: 33  time: 4.43  loss: 2.643432  acc: 0.98347\n",
      "epoch: 34  time: 4.64  loss: 2.162656  acc: 0.98743\n",
      "epoch: 35  time: 5.42  loss: 2.429965  acc: 0.98545\n",
      "epoch: 36  time: 4.90  loss: 3.209426  acc: 0.97487\n",
      "epoch: 37  time: 5.22  loss: 3.026610  acc: 0.98611\n",
      "epoch: 38  time: 6.03  loss: 2.681487  acc: 0.98479\n",
      "epoch: 39  time: 4.93  loss: 2.478416  acc: 0.98545\n",
      "epoch: 40  time: 5.34  loss: 3.416425  acc: 0.97685\n",
      "epoch: 41  time: 4.82  loss: 2.797636  acc: 0.98280\n",
      "epoch: 42  time: 4.72  loss: 2.935945  acc: 0.98479\n",
      "epoch: 43  time: 2.95  loss: 3.606531  acc: 0.97884\n",
      "epoch: 44  time: 2.93  loss: 4.569255  acc: 0.96693\n",
      "epoch: 45  time: 2.96  loss: 3.907775  acc: 0.97487\n",
      "epoch: 46  time: 3.13  loss: 2.552981  acc: 0.98148\n",
      "epoch: 47  time: 3.19  loss: 2.315142  acc: 0.98611\n",
      "epoch: 48  time: 3.46  loss: 2.104837  acc: 0.98810\n",
      "epoch: 49  time: 3.26  loss: 1.748411  acc: 0.98677\n",
      "epoch: 50  time: 3.32  loss: 1.693368  acc: 0.99008\n",
      "epoch: 51  time: 3.64  loss: 1.698347  acc: 0.98942\n",
      "epoch: 52  time: 3.08  loss: 2.256833  acc: 0.98677\n",
      "epoch: 53  time: 3.01  loss: 2.205877  acc: 0.98677\n",
      "epoch: 54  time: 3.10  loss: 2.535206  acc: 0.98214\n",
      "epoch: 55  time: 2.96  loss: 2.112140  acc: 0.98810\n",
      "epoch: 56  time: 2.95  loss: 2.643041  acc: 0.98280\n",
      "epoch: 57  time: 3.45  loss: 2.292314  acc: 0.98545\n",
      "epoch: 58  time: 3.79  loss: 2.507526  acc: 0.98677\n",
      "epoch: 59  time: 3.52  loss: 3.365688  acc: 0.98016\n",
      "epoch: 60  time: 3.46  loss: 6.740020  acc: 0.95635\n",
      "epoch: 61  time: 3.16  loss: 3.267987  acc: 0.98413\n",
      "epoch: 62  time: 2.96  loss: 4.560668  acc: 0.96825\n",
      "epoch: 63  time: 3.36  loss: 3.533696  acc: 0.97884\n",
      "epoch: 64  time: 3.30  loss: 2.483647  acc: 0.98743\n",
      "epoch: 65  time: 3.00  loss: 2.674723  acc: 0.98611\n",
      "epoch: 66  time: 3.02  loss: 2.044437  acc: 0.98810\n",
      "epoch: 67  time: 3.00  loss: 2.121268  acc: 0.98810\n",
      "epoch: 68  time: 2.99  loss: 1.936290  acc: 0.98810\n",
      "epoch: 69  time: 2.99  loss: 4.156204  acc: 0.97354\n",
      "epoch: 70  time: 3.03  loss: 3.966924  acc: 0.97222\n",
      "epoch: 71  time: 3.51  loss: 3.561581  acc: 0.97751\n",
      "epoch: 72  time: 3.50  loss: 2.947914  acc: 0.98347\n",
      "epoch: 73  time: 3.37  loss: 2.867348  acc: 0.98545\n",
      "epoch: 74  time: 2.99  loss: 2.198047  acc: 0.98611\n",
      "epoch: 75  time: 3.66  loss: 2.468747  acc: 0.98545\n",
      "epoch: 76  time: 3.81  loss: 1.825205  acc: 0.98611\n",
      "epoch: 77  time: 3.47  loss: 2.043884  acc: 0.98810\n",
      "epoch: 78  time: 3.18  loss: 2.027307  acc: 0.98942\n",
      "epoch: 79  time: 3.02  loss: 3.301607  acc: 0.98347\n",
      "epoch: 80  time: 2.96  loss: 3.773133  acc: 0.97817\n",
      "done.\n",
      "predict_acc :  0.8594059405940594\n",
      "train_data: 1513 , train_label: 1513\n",
      "test_data: 504 , test_label: 504\n",
      "LSTMClassifier_3NN\n",
      "LSTM\n",
      "Linear\n",
      "Linear\n",
      "Linear\n",
      "LogSoftmax\n",
      "epoch: 1  time: 3.06  loss: 35.736979  acc: 0.48909\n",
      "epoch: 2  time: 3.17  loss: 31.987372  acc: 0.64442\n",
      "epoch: 3  time: 3.40  loss: 21.830831  acc: 0.83146\n",
      "epoch: 4  time: 3.15  loss: 18.369896  acc: 0.86715\n",
      "epoch: 5  time: 3.12  loss: 16.521949  acc: 0.88367\n",
      "epoch: 6  time: 2.94  loss: 14.910447  acc: 0.89888\n",
      "epoch: 7  time: 2.96  loss: 13.240259  acc: 0.91937\n",
      "epoch: 8  time: 3.08  loss: 12.599343  acc: 0.91474\n",
      "epoch: 9  time: 3.05  loss: 11.581768  acc: 0.92664\n",
      "epoch: 10  time: 2.91  loss: 11.151907  acc: 0.93721\n",
      "epoch: 11  time: 2.90  loss: 9.621827  acc: 0.93853\n",
      "epoch: 12  time: 2.90  loss: 8.544007  acc: 0.94448\n",
      "epoch: 13  time: 2.91  loss: 7.003400  acc: 0.95440\n",
      "epoch: 14  time: 2.94  loss: 6.079530  acc: 0.96167\n",
      "epoch: 15  time: 2.91  loss: 6.010069  acc: 0.96233\n",
      "epoch: 16  time: 2.94  loss: 5.746750  acc: 0.95968\n",
      "epoch: 17  time: 3.06  loss: 5.042281  acc: 0.96894\n",
      "epoch: 18  time: 2.89  loss: 4.585169  acc: 0.97158\n",
      "epoch: 19  time: 2.95  loss: 4.198504  acc: 0.97621\n",
      "epoch: 20  time: 3.03  loss: 3.830024  acc: 0.97356\n",
      "epoch: 21  time: 2.94  loss: 3.258428  acc: 0.98017\n",
      "epoch: 22  time: 2.90  loss: 3.177402  acc: 0.98017\n",
      "epoch: 23  time: 2.89  loss: 5.445135  acc: 0.96497\n",
      "epoch: 24  time: 2.87  loss: 4.828490  acc: 0.97290\n",
      "epoch: 25  time: 2.90  loss: 3.739071  acc: 0.97356\n",
      "epoch: 26  time: 2.94  loss: 2.980539  acc: 0.98282\n",
      "epoch: 27  time: 2.89  loss: 3.145148  acc: 0.98083\n",
      "epoch: 28  time: 2.91  loss: 3.027152  acc: 0.98480\n",
      "epoch: 29  time: 2.91  loss: 2.697300  acc: 0.98348\n",
      "epoch: 30  time: 2.91  loss: 5.058941  acc: 0.96497\n",
      "epoch: 31  time: 2.90  loss: 4.742658  acc: 0.96960\n",
      "epoch: 32  time: 2.90  loss: 3.036937  acc: 0.97951\n",
      "epoch: 33  time: 2.90  loss: 4.243691  acc: 0.96960\n",
      "epoch: 34  time: 3.35  loss: 3.147902  acc: 0.98215\n",
      "epoch: 35  time: 3.22  loss: 2.013909  acc: 0.98876\n",
      "epoch: 36  time: 2.90  loss: 2.220699  acc: 0.98744\n",
      "epoch: 37  time: 2.98  loss: 1.642799  acc: 0.99141\n",
      "epoch: 38  time: 2.89  loss: 1.309443  acc: 0.99273\n",
      "epoch: 39  time: 2.90  loss: 1.382569  acc: 0.98942\n",
      "epoch: 40  time: 3.04  loss: 1.178838  acc: 0.99339\n",
      "epoch: 41  time: 2.92  loss: 1.153678  acc: 0.99207\n",
      "epoch: 42  time: 2.90  loss: 1.014753  acc: 0.99339\n",
      "epoch: 43  time: 2.91  loss: 1.024001  acc: 0.99207\n",
      "epoch: 44  time: 2.95  loss: 1.306846  acc: 0.99141\n",
      "epoch: 45  time: 2.91  loss: 1.903687  acc: 0.98942\n",
      "epoch: 46  time: 2.90  loss: 1.170219  acc: 0.99207\n",
      "epoch: 47  time: 2.91  loss: 1.236624  acc: 0.99471\n",
      "epoch: 48  time: 2.92  loss: 1.084931  acc: 0.99207\n",
      "epoch: 49  time: 2.91  loss: 1.761833  acc: 0.98942\n",
      "epoch: 50  time: 2.94  loss: 2.851028  acc: 0.97753\n",
      "epoch: 51  time: 2.94  loss: 3.712044  acc: 0.97356\n",
      "epoch: 52  time: 2.91  loss: 3.396964  acc: 0.98149\n",
      "epoch: 53  time: 2.92  loss: 3.370902  acc: 0.97951\n",
      "epoch: 54  time: 2.92  loss: 1.924317  acc: 0.98810\n",
      "epoch: 55  time: 3.06  loss: 1.178534  acc: 0.99207\n",
      "epoch: 56  time: 3.20  loss: 1.113500  acc: 0.99405\n",
      "epoch: 57  time: 3.25  loss: 1.950695  acc: 0.98810\n",
      "epoch: 58  time: 3.31  loss: 1.312752  acc: 0.99273\n",
      "epoch: 59  time: 3.22  loss: 0.983982  acc: 0.99405\n",
      "epoch: 60  time: 3.20  loss: 0.996613  acc: 0.99339\n",
      "epoch: 61  time: 3.07  loss: 2.120324  acc: 0.98744\n",
      "epoch: 62  time: 3.31  loss: 2.386898  acc: 0.98348\n",
      "epoch: 63  time: 3.24  loss: 2.303449  acc: 0.98149\n",
      "epoch: 64  time: 3.34  loss: 1.470022  acc: 0.99075\n",
      "epoch: 65  time: 3.34  loss: 2.102716  acc: 0.98546\n",
      "epoch: 66  time: 3.25  loss: 1.703884  acc: 0.98942\n",
      "epoch: 67  time: 3.27  loss: 2.483578  acc: 0.98546\n",
      "epoch: 68  time: 3.07  loss: 3.777642  acc: 0.97356\n",
      "epoch: 69  time: 3.15  loss: 2.756753  acc: 0.98282\n",
      "epoch: 70  time: 3.01  loss: 1.848976  acc: 0.98942\n",
      "epoch: 71  time: 3.05  loss: 1.996811  acc: 0.98612\n",
      "epoch: 72  time: 2.92  loss: 1.227762  acc: 0.99141\n",
      "epoch: 73  time: 3.11  loss: 0.972148  acc: 0.99339\n",
      "epoch: 74  time: 3.16  loss: 0.792895  acc: 0.99339\n",
      "epoch: 75  time: 3.77  loss: 0.795726  acc: 0.99405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 76  time: 3.35  loss: 0.766761  acc: 0.99405\n",
      "epoch: 77  time: 3.08  loss: 0.760848  acc: 0.99273\n",
      "epoch: 78  time: 3.10  loss: 0.685849  acc: 0.99339\n",
      "epoch: 79  time: 3.43  loss: 0.665472  acc: 0.99405\n",
      "epoch: 80  time: 3.16  loss: 0.845900  acc: 0.99273\n",
      "done.\n",
      "predict_acc :  0.8412698412698413\n",
      "train_data: 1513 , train_label: 1513\n",
      "test_data: 504 , test_label: 504\n",
      "LSTMClassifier_3NN\n",
      "LSTM\n",
      "Linear\n",
      "Linear\n",
      "Linear\n",
      "LogSoftmax\n",
      "epoch: 1  time: 3.48  loss: 35.734781  acc: 0.51950\n",
      "epoch: 2  time: 3.12  loss: 30.329005  acc: 0.66424\n",
      "epoch: 3  time: 3.15  loss: 21.227303  acc: 0.83741\n",
      "epoch: 4  time: 3.21  loss: 16.912661  acc: 0.87046\n",
      "epoch: 5  time: 3.13  loss: 14.845523  acc: 0.89028\n",
      "epoch: 6  time: 3.07  loss: 14.615031  acc: 0.90086\n",
      "epoch: 7  time: 3.11  loss: 13.339651  acc: 0.91011\n",
      "epoch: 8  time: 3.12  loss: 13.401929  acc: 0.91276\n",
      "epoch: 9  time: 3.96  loss: 10.303759  acc: 0.92531\n",
      "epoch: 10  time: 4.45  loss: 9.399614  acc: 0.93325\n",
      "epoch: 11  time: 3.96  loss: 8.883366  acc: 0.94316\n",
      "epoch: 12  time: 4.38  loss: 8.715463  acc: 0.94184\n",
      "epoch: 13  time: 4.25  loss: 7.355997  acc: 0.94911\n",
      "epoch: 14  time: 3.62  loss: 6.300653  acc: 0.96431\n",
      "epoch: 15  time: 3.32  loss: 4.753957  acc: 0.97555\n",
      "epoch: 16  time: 3.08  loss: 5.375172  acc: 0.97026\n",
      "epoch: 17  time: 3.14  loss: 4.719583  acc: 0.97026\n",
      "epoch: 18  time: 3.20  loss: 4.899565  acc: 0.97356\n",
      "epoch: 19  time: 3.25  loss: 5.299385  acc: 0.96761\n",
      "epoch: 20  time: 3.15  loss: 3.848805  acc: 0.97885\n",
      "epoch: 21  time: 3.12  loss: 4.060163  acc: 0.97885\n",
      "epoch: 22  time: 3.16  loss: 5.577516  acc: 0.96629\n",
      "epoch: 23  time: 3.15  loss: 3.882935  acc: 0.97753\n",
      "epoch: 24  time: 3.52  loss: 3.448274  acc: 0.98017\n",
      "epoch: 25  time: 3.39  loss: 3.346144  acc: 0.98282\n",
      "epoch: 26  time: 3.14  loss: 2.727964  acc: 0.98744\n",
      "epoch: 27  time: 4.16  loss: 2.494914  acc: 0.98810\n",
      "epoch: 28  time: 4.44  loss: 2.924254  acc: 0.98149\n",
      "epoch: 29  time: 3.52  loss: 3.702632  acc: 0.98149\n",
      "epoch: 30  time: 3.57  loss: 3.227505  acc: 0.98348\n",
      "epoch: 31  time: 3.27  loss: 2.601039  acc: 0.98876\n",
      "epoch: 32  time: 3.41  loss: 2.823159  acc: 0.98414\n",
      "epoch: 33  time: 3.34  loss: 3.979228  acc: 0.97885\n",
      "epoch: 34  time: 4.92  loss: 5.377273  acc: 0.96629\n",
      "epoch: 35  time: 4.15  loss: 4.981056  acc: 0.96629\n",
      "epoch: 36  time: 3.39  loss: 4.233550  acc: 0.97555\n",
      "epoch: 37  time: 3.86  loss: 3.579381  acc: 0.97951\n",
      "epoch: 38  time: 3.33  loss: 2.580142  acc: 0.98480\n",
      "epoch: 39  time: 3.51  loss: 2.602519  acc: 0.98744\n",
      "epoch: 40  time: 3.33  loss: 1.983345  acc: 0.98612\n",
      "epoch: 41  time: 3.37  loss: 1.837459  acc: 0.99009\n",
      "epoch: 42  time: 3.17  loss: 1.622910  acc: 0.99009\n",
      "epoch: 43  time: 3.34  loss: 1.755440  acc: 0.98942\n",
      "epoch: 44  time: 3.49  loss: 2.063505  acc: 0.98810\n",
      "epoch: 45  time: 3.12  loss: 2.698855  acc: 0.98678\n",
      "epoch: 46  time: 3.21  loss: 2.866772  acc: 0.98414\n",
      "epoch: 47  time: 3.11  loss: 3.852651  acc: 0.97621\n",
      "epoch: 48  time: 3.08  loss: 1.692212  acc: 0.99207\n",
      "epoch: 49  time: 2.97  loss: 1.423540  acc: 0.99273\n",
      "epoch: 50  time: 2.96  loss: 1.340397  acc: 0.99273\n",
      "epoch: 51  time: 3.34  loss: 1.493196  acc: 0.99273\n",
      "epoch: 52  time: 4.02  loss: 1.592678  acc: 0.99009\n",
      "epoch: 53  time: 3.49  loss: 1.639425  acc: 0.99075\n",
      "epoch: 54  time: 3.16  loss: 1.359943  acc: 0.99207\n",
      "epoch: 55  time: 3.02  loss: 1.278850  acc: 0.99273\n",
      "epoch: 56  time: 3.01  loss: 1.270677  acc: 0.99207\n",
      "epoch: 57  time: 3.34  loss: 1.295910  acc: 0.99273\n",
      "epoch: 58  time: 3.63  loss: 1.231284  acc: 0.99273\n",
      "epoch: 59  time: 3.76  loss: 1.229867  acc: 0.99273\n",
      "epoch: 60  time: 3.47  loss: 1.449332  acc: 0.99273\n",
      "epoch: 61  time: 3.11  loss: 2.546501  acc: 0.98678\n",
      "epoch: 62  time: 3.23  loss: 3.392976  acc: 0.98017\n",
      "epoch: 63  time: 3.50  loss: 5.092572  acc: 0.96827\n",
      "epoch: 64  time: 3.68  loss: 2.301167  acc: 0.98546\n",
      "epoch: 65  time: 3.30  loss: 2.021290  acc: 0.98876\n",
      "epoch: 66  time: 3.03  loss: 5.717925  acc: 0.95902\n",
      "epoch: 67  time: 3.03  loss: 6.851592  acc: 0.95373\n",
      "epoch: 68  time: 3.02  loss: 4.775394  acc: 0.96629\n",
      "epoch: 69  time: 3.02  loss: 5.040874  acc: 0.96563\n",
      "epoch: 70  time: 3.95  loss: 2.209199  acc: 0.98942\n",
      "epoch: 71  time: 3.73  loss: 2.096067  acc: 0.98612\n",
      "epoch: 72  time: 3.62  loss: 2.179624  acc: 0.98546\n",
      "epoch: 73  time: 3.92  loss: 1.513665  acc: 0.99207\n",
      "epoch: 74  time: 4.06  loss: 1.325918  acc: 0.99273\n",
      "epoch: 75  time: 3.09  loss: 1.228079  acc: 0.99207\n",
      "epoch: 76  time: 3.00  loss: 1.250656  acc: 0.99273\n",
      "epoch: 77  time: 2.98  loss: 1.249468  acc: 0.99207\n",
      "epoch: 78  time: 3.00  loss: 1.240251  acc: 0.99273\n",
      "epoch: 79  time: 2.97  loss: 1.253579  acc: 0.99273\n",
      "epoch: 80  time: 3.86  loss: 1.131284  acc: 0.99207\n",
      "done.\n",
      "predict_acc :  0.8571428571428571\n",
      "train_data: 1513 , train_label: 1513\n",
      "test_data: 504 , test_label: 504\n",
      "LSTMClassifier_3NN\n",
      "LSTM\n",
      "Linear\n",
      "Linear\n",
      "Linear\n",
      "LogSoftmax\n",
      "epoch: 1  time: 3.09  loss: 35.714853  acc: 0.51751\n",
      "epoch: 2  time: 2.92  loss: 32.244174  acc: 0.62062\n",
      "epoch: 3  time: 3.08  loss: 22.326461  acc: 0.81956\n",
      "epoch: 4  time: 3.07  loss: 17.958670  acc: 0.86781\n",
      "epoch: 5  time: 4.04  loss: 15.174276  acc: 0.88962\n",
      "epoch: 6  time: 4.93  loss: 12.942388  acc: 0.90879\n",
      "epoch: 7  time: 3.77  loss: 12.404427  acc: 0.90879\n",
      "epoch: 8  time: 3.18  loss: 10.033518  acc: 0.93325\n",
      "epoch: 9  time: 3.62  loss: 10.438724  acc: 0.92994\n",
      "epoch: 10  time: 3.39  loss: 9.194585  acc: 0.94250\n",
      "epoch: 11  time: 3.28  loss: 8.915937  acc: 0.94184\n",
      "epoch: 12  time: 3.86  loss: 7.819495  acc: 0.95109\n",
      "epoch: 13  time: 3.53  loss: 8.270092  acc: 0.94911\n",
      "epoch: 14  time: 3.35  loss: 9.534588  acc: 0.94382\n",
      "epoch: 15  time: 4.02  loss: 6.822175  acc: 0.96167\n",
      "epoch: 16  time: 3.56  loss: 7.326821  acc: 0.95638\n",
      "epoch: 17  time: 3.33  loss: 6.074464  acc: 0.96497\n",
      "epoch: 18  time: 3.17  loss: 5.893986  acc: 0.96761\n",
      "epoch: 19  time: 3.47  loss: 5.120317  acc: 0.96695\n",
      "epoch: 20  time: 3.57  loss: 6.672003  acc: 0.95902\n",
      "epoch: 21  time: 3.64  loss: 4.913816  acc: 0.97555\n",
      "epoch: 22  time: 3.47  loss: 4.418544  acc: 0.97555\n",
      "epoch: 23  time: 3.27  loss: 4.372368  acc: 0.97621\n",
      "epoch: 24  time: 3.14  loss: 4.154127  acc: 0.97488\n",
      "epoch: 25  time: 3.93  loss: 4.331846  acc: 0.97488\n",
      "epoch: 26  time: 3.31  loss: 5.948394  acc: 0.96497\n",
      "epoch: 27  time: 3.07  loss: 5.130192  acc: 0.97224\n",
      "epoch: 28  time: 3.41  loss: 5.256403  acc: 0.96695\n",
      "epoch: 29  time: 3.39  loss: 4.602908  acc: 0.97555\n",
      "epoch: 30  time: 3.77  loss: 3.814709  acc: 0.97951\n",
      "epoch: 31  time: 3.49  loss: 4.087396  acc: 0.97753\n",
      "epoch: 32  time: 3.14  loss: 3.801598  acc: 0.97885\n",
      "epoch: 33  time: 4.12  loss: 5.013047  acc: 0.97092\n",
      "epoch: 34  time: 3.25  loss: 4.186798  acc: 0.97555\n",
      "epoch: 35  time: 3.21  loss: 2.814703  acc: 0.98546\n",
      "epoch: 36  time: 3.03  loss: 2.717268  acc: 0.98678\n",
      "epoch: 37  time: 3.12  loss: 3.111707  acc: 0.98480\n",
      "epoch: 38  time: 3.13  loss: 4.303055  acc: 0.97819\n",
      "epoch: 39  time: 3.08  loss: 4.729141  acc: 0.97158\n",
      "epoch: 40  time: 3.06  loss: 3.717694  acc: 0.98282\n",
      "epoch: 41  time: 3.03  loss: 3.352561  acc: 0.98149\n",
      "epoch: 42  time: 3.02  loss: 2.577576  acc: 0.98612\n",
      "epoch: 43  time: 2.97  loss: 2.445532  acc: 0.98810\n",
      "epoch: 44  time: 3.05  loss: 2.543634  acc: 0.98744\n",
      "epoch: 45  time: 3.14  loss: 2.909324  acc: 0.98612\n",
      "epoch: 46  time: 3.33  loss: 2.317793  acc: 0.98942\n",
      "epoch: 47  time: 3.19  loss: 2.523885  acc: 0.98678\n",
      "epoch: 48  time: 3.00  loss: 2.317478  acc: 0.98810\n",
      "epoch: 49  time: 3.04  loss: 2.609370  acc: 0.98744\n",
      "epoch: 50  time: 3.07  loss: 2.820010  acc: 0.98678\n",
      "epoch: 51  time: 3.05  loss: 3.098501  acc: 0.98414\n",
      "epoch: 52  time: 3.23  loss: 2.760445  acc: 0.98546\n",
      "epoch: 53  time: 2.96  loss: 3.066527  acc: 0.98414\n",
      "epoch: 54  time: 2.94  loss: 3.078981  acc: 0.98282\n",
      "epoch: 55  time: 2.99  loss: 3.245071  acc: 0.98215\n",
      "epoch: 56  time: 2.99  loss: 2.744902  acc: 0.98414\n",
      "epoch: 57  time: 2.98  loss: 2.130842  acc: 0.98942\n",
      "epoch: 58  time: 3.15  loss: 3.021022  acc: 0.98546\n",
      "epoch: 59  time: 3.38  loss: 3.374409  acc: 0.98149\n",
      "epoch: 60  time: 3.30  loss: 3.312918  acc: 0.98017\n",
      "epoch: 61  time: 3.40  loss: 5.398557  acc: 0.96894\n",
      "epoch: 62  time: 3.59  loss: 4.452872  acc: 0.97422\n",
      "epoch: 63  time: 3.77  loss: 2.970849  acc: 0.98348\n",
      "epoch: 64  time: 3.78  loss: 3.920575  acc: 0.98083\n",
      "epoch: 65  time: 3.27  loss: 2.974869  acc: 0.98282\n",
      "epoch: 66  time: 3.09  loss: 2.509043  acc: 0.98612\n",
      "epoch: 67  time: 3.52  loss: 2.835411  acc: 0.98348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 68  time: 2.97  loss: 2.543274  acc: 0.98612\n",
      "epoch: 69  time: 3.21  loss: 2.307797  acc: 0.98810\n",
      "epoch: 70  time: 3.37  loss: 2.355095  acc: 0.98612\n",
      "epoch: 71  time: 2.97  loss: 2.284992  acc: 0.98876\n",
      "epoch: 72  time: 3.16  loss: 1.981097  acc: 0.98876\n",
      "epoch: 73  time: 3.55  loss: 3.270252  acc: 0.98282\n",
      "epoch: 74  time: 3.29  loss: 2.718897  acc: 0.98546\n",
      "epoch: 75  time: 3.10  loss: 1.960264  acc: 0.98810\n",
      "epoch: 76  time: 2.97  loss: 1.752205  acc: 0.99009\n",
      "epoch: 77  time: 3.04  loss: 1.415171  acc: 0.99141\n",
      "epoch: 78  time: 2.93  loss: 1.589212  acc: 0.99075\n",
      "epoch: 79  time: 2.93  loss: 1.797113  acc: 0.98744\n",
      "epoch: 80  time: 2.91  loss: 1.831667  acc: 0.98876\n",
      "done.\n",
      "predict_acc :  0.8611111111111112\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch : score\n",
      "0 : 0.8594059405940594\n",
      "1 : 0.8412698412698413\n",
      "2 : 0.8571428571428571\n",
      "3 : 0.8611111111111112\n",
      "[[0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0]]\n",
      "predict_ave :  0.8547324375294673\n",
      "0.8539805319817697\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import copy\n",
    "import random\n",
    "from bert_juman import BertWithJumanModel\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from pyknp import Juman\n",
    "jumanpp = Juman()\n",
    "\n",
    "#設定\n",
    "change_flag = 1#誤採点を作成、０で作らない、１で作る\n",
    "val_num = 4#クロスバリデーションの分割数\n",
    "epoch_num = 100\n",
    "miss = 20#誤採点の数\n",
    "change_seeds = []\n",
    "for i in range(10):#シャッフルのseed設定、繰り返す回数作成\n",
    "    change_seeds.append(random.randint(1,1000))\n",
    "\n",
    "#初期化\n",
    "ave_acc = 0\n",
    "\n",
    "acc_val = np.zeros((len(change_seeds), val_num), dtype = np.float64)\n",
    "ave_acc = np.zeros(len(change_seeds), dtype = np.float64)\n",
    "new_student_answer_list = np.zeros((len(change_seeds), len(student_list)), dtype = np.float64)\n",
    "\n",
    "all_ans_label = []\n",
    "all_pred_point = []\n",
    "text_pos_memo = []\n",
    "change_num = []\n",
    "new_student_list = []\n",
    "\n",
    "for i in range(len(change_seeds)):\n",
    "    all_ans_label.append([])\n",
    "    all_pred_point.append([])\n",
    "    new_student_list.append([])\n",
    "    text_pos_memo.append([])\n",
    "    change_num.append([])\n",
    "\n",
    "#モデルの読み込み\n",
    "if model_select == 3:\n",
    "    if laboro_model == 0:\n",
    "        EMBEDDING_DIM = 768\n",
    "        bert_vec = make_bert_vec(qnum, val_num)\n",
    "    elif laboro_model == 1:\n",
    "        EMBEDDING_DIM = 768\n",
    "        bert_vec = make_bert_vec_laboro(qnum, val_num, laboro_model)\n",
    "    elif laboro_model == 2:\n",
    "        EMBEDDING_DIM = 1024\n",
    "        bert_vec = make_bert_vec_laboro(qnum, val_num, laboro_model)\n",
    "\n",
    "    \n",
    "text_pos = []\n",
    "for i in range(len(student_list)):\n",
    "    text_pos.append(i)\n",
    "    \n",
    "if change_flag == 1:#誤採点作成\n",
    "    change_student_answer_list, change_pos = make_change_answer(student_answer_list, miss)\n",
    "elif change_flag == 0:#誤採点を作らない\n",
    "    change_pos = np.zeros(len(student_list), dtype = np.int64)\n",
    "    change_student_answer_list = copy.copy(student_answer_list)\n",
    "    \n",
    "change_student_answer_list = my_shuffle(change_seeds[0], change_student_answer_list)\n",
    "    \n",
    "\n",
    "for change_id, change_seed in enumerate(change_seeds):\n",
    "    \n",
    "    train_val_list= []\n",
    "    test_val_list= []\n",
    "    train_val_label= []\n",
    "    test_val_label= []\n",
    "\n",
    "    lstm_vec = []    \n",
    "\n",
    "    #データシャッフル\n",
    "    student_list = my_shuffle(change_seed, student_list)\n",
    "    change_student_answer_list = my_shuffle(change_seed, change_student_answer_list)\n",
    "    change_pos = my_shuffle(change_seed, change_pos)\n",
    "    bert_vec = my_shuffle(change_seed, bert_vec)\n",
    "    \n",
    "    if change_id == 0:\n",
    "        base_change_pos = copy.copy(change_pos)#誤採点の最初の位置を記憶\n",
    "    else:#初回のデータの並びを記憶\n",
    "        text_pos = my_shuffle(seed, text_pos)\n",
    "            \n",
    "    #シャッフルごとにデータ保存\n",
    "    new_student_list[change_id] = copy.copy(student_list)\n",
    "    text_pos_memo[change_id] = copy.copy(text_pos)\n",
    "    new_student_answer_list[change_id] = copy.copy(change_student_answer_list)\n",
    "    change_num[change_id] = copy.copy(change_pos)\n",
    "    \n",
    "    if model_select == 3:\n",
    "        train_val_vec, test_val_vec = self_cross_val(bert_vec, val_num)\n",
    "\n",
    "    #データをクロスバリデーション用に分割\n",
    "    train_val_list, test_val_list = self_cross_val(student_list, val_num)\n",
    "    train_val_label, test_val_label = self_cross_val(change_student_answer_list, val_num)\n",
    "\n",
    "    #cross_valdation\n",
    "    for val_epo in range(val_num):\n",
    "        train_list = copy.copy(train_val_list[val_epo])\n",
    "        train_label = copy.copy(train_val_label[val_epo])\n",
    "        test_list = copy.copy(test_val_list[val_epo])\n",
    "        test_label = copy.copy(test_val_label[val_epo])\n",
    "       \n",
    "\n",
    "\n",
    "        all_list = train_list+test_list\n",
    "        all_label = train_label+test_label\n",
    "\n",
    "        # データセットの準備\n",
    "        index_datasets_title = []\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "        elif model_select == 3:\n",
    "            index_datasets_title = train_val_vec[val_epo] + test_val_vec[val_epo]\n",
    "\n",
    "        elif model_select == 5:\n",
    "            index_datasets_title = train_val_vec[val_epo] + test_val_vec[val_epo]\n",
    "\n",
    "\n",
    "        all_vec_data = []\n",
    "        for vec in index_datasets_title:\n",
    "            vec = torch.tensor(vec)\n",
    "            all_vec_data.append(vec)\n",
    "\n",
    "        #分割\n",
    "        train_data = all_vec_data[:len(train_list)]\n",
    "        test_data = all_vec_data[len(train_list):]\n",
    "\n",
    "        print('train_data:',len(train_data),', train_label:',len(train_label))\n",
    "        print('test_data:',len(test_data),', test_label:',len(test_label))\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------\n",
    "        #学習モデル\n",
    "        HIDDEN_DIM = 128\n",
    "        TAG_SIZE = 2\n",
    "\n",
    "        model = LSTMClassifier_3NN(EMBEDDING_DIM, HIDDEN_DIM, TAG_SIZE).to(device)  # to(device)でモデルがGPU対応する\n",
    "\n",
    "        for m in model.modules():\n",
    "            print(m.__class__.__name__)\n",
    "            weights_init(m)\n",
    "\n",
    "        loss_function = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        losses_epoch = []\n",
    "        losses_batch = []\n",
    "        accs = []\n",
    "        weights_init(model)\n",
    "\n",
    "        model.train() #学習モード\n",
    "        import time\n",
    "        now_time = time.time()\n",
    "        for epoch in range(epoch_num):\n",
    "\n",
    "            title_batch, category_batch = train2batch(train_data, train_label, 30)\n",
    "\n",
    "            batch_count = 0\n",
    "            all_loss = 0\n",
    "            all_acc = 0\n",
    "\n",
    "            for i in range(len(title_batch)):\n",
    "\n",
    "                title_batch_pad = rnn.pad_sequence(title_batch[i], batch_first=True)  # padding\n",
    "    #             title_batch_pad = title_batch[i]\n",
    "\n",
    "                batch_loss = 0\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬させるtensorはGPUで処理させるためdevice=にGPUをセット\n",
    "                title_tensor = torch.tensor(title_batch_pad, device=device)\n",
    "\n",
    "                # category_tensor.size() = (batch_size × 1)なので、squeeze()\n",
    "                category_tensor = torch.tensor(category_batch[i], device=device).squeeze()\n",
    "\n",
    "                out, out_vec = model(title_tensor)\n",
    "\n",
    "                batch_loss = loss_function(out, category_tensor)\n",
    "                batch_loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_count += len(title_batch_pad)\n",
    "                all_loss += batch_loss.item()\n",
    "                losses_batch.append(batch_loss.item())\n",
    "\n",
    "                _, predicts = torch.max(out, 1)\n",
    "\n",
    "                for j, ans in enumerate(category_tensor):\n",
    "                    if predicts[j].item() == ans.item():\n",
    "                        all_acc += 1\n",
    "\n",
    "            losses_epoch.append(all_loss)\n",
    "\n",
    "            acc = all_acc/len(train_data)\n",
    "            accs.append(acc)\n",
    "\n",
    "            before_time  = now_time\n",
    "            now_time = time.time()\n",
    "            loss_time = now_time - before_time\n",
    "            print('epoch: %d  time: %.2f  loss: %.6f  acc: %.5f' %(epoch+1, loss_time, all_loss, acc))\n",
    "\n",
    "        print(\"done.\")\n",
    "\n",
    "\n",
    "#         print_graph(losses_epoch, accs)\n",
    "\n",
    "\n",
    "        # test\n",
    "        test_num = len(test_data)\n",
    "        a = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            title_batch, category_batch = train2batch(test_data, test_label, 30)\n",
    "\n",
    "            for i in range(len(title_batch)):\n",
    "                title_batch_pad = rnn.pad_sequence(title_batch[i], batch_first=True)  # padding\n",
    "                title_tensor = torch.tensor(title_batch_pad, device=device)\n",
    "\n",
    "                category_tensor = torch.tensor(category_batch[i], device=device)\n",
    "\n",
    "                out, vec_out = model(title_tensor)\n",
    "                for per, vec in zip(out, vec_out):\n",
    "                    lstm_vec.append(vec)\n",
    "                    all_pred_point[change_id].append(per)\n",
    "\n",
    "                _, predicts = torch.max(out, 1)\n",
    "\n",
    "                for j, ans in enumerate(category_tensor):\n",
    "                    all_ans_label[change_id].append(predicts[j].item())\n",
    "                    if predicts[j].item() == ans.item():\n",
    "                        a += 1\n",
    "\n",
    "            epo_acc = a/test_num\n",
    "\n",
    "        print(\"predict_acc : \", epo_acc)\n",
    "        acc_val[change_id][val_epo] = epo_acc\n",
    "\n",
    "        ave_acc[change_id] += epo_acc\n",
    "\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"epoch : score\")\n",
    "    for i, score in enumerate(acc_val[change_id]):\n",
    "        print(i, \":\", score)\n",
    "        \n",
    "    print(all_ans_label)\n",
    "\n",
    "    ave_acc[change_id] /= val_num\n",
    "    print(\"predict_ave : \", ave_acc[change_id])\n",
    "\n",
    "average_acc = 0\n",
    "for i in ave_acc:\n",
    "    average_acc += i\n",
    "    \n",
    "print(average_acc/len(change_seeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "検出回数： 14\n",
      "==================================================\n",
      "12 　　　 × 抵抗が電熱線が1つのときより大きくなり、電流がながれにくくなったから\n",
      "68 　　　 × 電熱線が2つになり抵抗が2倍になり電流が流れにくくなったから。\n",
      "77 　　　 ◯ 直列回路につないだときの電流の大きさは小さく、抵抗は大きいから。\n",
      "82 　　　 × 直列回路で2つの電熱線てつないだことで抵抗の大きさが2倍になり、電流がさらに流れにくくなったから。\n",
      "94 誤採点 × 抵抗が大きく、電流が流れにくくなったから。\n",
      "105 　　　 ◯ 抵抗が大きく、電流が電熱線が2つのときより、小さいから\n",
      "164 誤採点 × 抵抗が大きくなり、電流が流れにくくなったから。\n",
      "231 　　　 ◯ ていこうが強く電流が流れにくいため。\n",
      "270 　　　 × 抵抗が大きいと電流がより流れにくくなるから。\n",
      "287 　　　 ◯ 2つにすることで1つにおくられる電流が小さくなり抵抗が大きくなるため\n",
      "338 　　　 × 電源電圧を8.0Vにしたことで抵抗が大きくなり電流が弱くなったから\n",
      "381 　　　 × 抵抗が大きくなり、電流が流れにくくなったため。\n",
      "400 　　　 ◯ 電流を抵抗する力が増えたから。\n",
      "405 　　　 ◯ 電流が流れても抵抗があるので伝わる電流が小さくなってしまうから。\n",
      "447 　　　 ◯ 電流の大きさは小さくなり、抵抗の大きさは大きくなるから。\n",
      "465 　　　 ◯ 抵抗が増えて、電流が少しさえぎられて小さくなったから。\n",
      "702 　　　 × 電熱線を直列につないだことで、全体の抵抗が大きくなったので電流が流れにくくなったから。\n",
      "750 　　　 × 抵抗が大きく、電流が流れにくかったから\n",
      "759 　　　 × 抵抗が大きくなって電流が流れにくくなったから\n",
      "761 　　　 × 抵抗が大きいと電流が流れる力は小さくなるため。\n",
      "841 　　　 ◯ 抵抗が2つになり、電流が流れにくくなったから、\n",
      "922 　　　 × 抵抗が増え電流の流れが遅くなるから\n",
      "983 　　　 × 抵抗の大きさが2倍になり、実験のときよりも電流が流れにくくなったため。\n",
      "1012 　　　 ◯ 電熱線の抵抗によって流れる電流が減るから。\n",
      "1114 　　　 ◯ 電熱線1つのときよりも、抵抗と電流の割合が抵抗の方が大きくなったため。\n",
      "1125 　　　 × 抵抗が大きくなり電流が流れにくくなったから。\n",
      "1140 　　　 ◯ 電流が小さくなったのに対して抵抗が大きくなったから。\n",
      "1171 誤採点 ◯ 図1の時より低抗が大きいので、電流が流れにくくなっているから。\n",
      "1174 　　　 × 抵抗が多く電流が流れづらかったから\n",
      "1220 　　　 × 抵抗が大きくなり、電流が小さくなるから。\n",
      "1228 　　　 × 抵抗の大きさが小さいため流れる電流の大きさも小さくなり、水の上昇温度は小さくなった\n",
      "1234 　　　 × 電熱線が増えると抵抗も1つのときより大きくなるので、電流が流れにくくなるから。\n",
      "1261 　　　 × 抵抗が大きくなったため、電流が流れにくくなったから。\n",
      "1290 誤採点 × 抵抗が大きくなり、電流が流れにくくなったから\n",
      "1305 　　　 × 電熱線を直列につなぐと抵抗の大きさが大きくなり、電流が流れにくくなるから\n",
      "1309 　　　 × 電熱線をふやすと抵抗も大きくなり、電流が小さくなるから。\n",
      "1407 　　　 × 抵抗が大きく、電流が少なかったから。\n",
      "1457 　　　 ◯ 電流が抵抗、..2つで半分になってしまっているから。\n",
      "1531 　　　 × 電圧が小さくなって抵抗が大きくなったので電流が通りにくくなった\n",
      "1535 　　　 × 電流が抵抗よりも低かったため電流が流れにくかったから。\n",
      "1613 　　　 × 抵抗が大きくなったため、水に加わる電流の大きさが少なくなったから。\n",
      "1641 　　　 × 抵抗が大きくて、電流があまり流れなかったから\n",
      "1665 　　　 × 抵抗の大きさは大きくなったので、電流がつたわりずらくなったから\n",
      "1853 　　　 × 抵抗が大きくなり、電流が流れにくくなったため。\n",
      "1887 　　　 ◯ 抵抗により電流が流れにくくなって温度が上がらなかったから。\n",
      "1955 誤採点 ◯ 同じ電流でも、抵抗が大きいと電圧が小さくなるから。\n",
      "2001 　　　 ◯ 抵抗により、電流の大きさが小さくなったから。\n",
      "誤採点検出回数： 47 5\n",
      "検出回数： 13\n",
      "==================================================\n",
      "1 誤採点 × 2つの電熱線を直列につないだことで、抵抗が大きくなり電流が流れにくくなってしまったから。\n",
      "25 　　　 × 抵抗が大きくなったため電流が水に流れづらくなったから\n",
      "46 　　　 × 2つの電熱線を使うため、低抗が大きくなり、流れる電流は小さくなるから。\n",
      "53 　　　 ◯ 抵抗が2つになって1つの電熱線に流れる電流が半分になってしまったから。\n",
      "163 　　　 × 抵抗が大きくなったため電流が流れにくくなり、温度が伝わりにくくなったから。\n",
      "189 　　　 ◯ 抵抗が大きく、電流があまり流れなかったから\n",
      "277 　　　 × 電熱線が数が増えたので、ていこうが大きくなり、電流の大きさが小さくなったから。\n",
      "305 誤採点 × 抵抗が大きくなり、電流が流れずらくなるため。\n",
      "343 　　　 × オームの法則から、抵抗が大きくなると、電流が大きくなるから。\n",
      "373 　　　 ◯ 電熱線が1つ増えたため、抵抗も増え電流が流れづらくなった。\n",
      "380 　　　 ◯ 抵抗が大きくなると電流が流れにくくなるから\n",
      "392 　　　 × 抵抗が大きくなり、電流が流れずらくなったから。\n",
      "402 誤採点 × 抵抗の大きさが同じ電熱線を直列につないだため、抵抗が大きくなり、電流が流れにくくなったから。\n",
      "425 　　　 ◯ 抵抗がおおきいため電流もそれほど強くなかった。\n",
      "461 　　　 × 直列回路は電流が等しくなるが、抵抗が大きくなったので電流が低くなり電力量が下がったから\n",
      "565 　　　 × 抵抗が大きくなると電流は流れにくくなる\n",
      "597 　　　 × 抵抗が、大きくなり電流が流れにくくなったため。\n",
      "619 　　　 × 低抗の大きさが大きくなって、電流が流れにくくなったから。\n",
      "695 誤採点 ◯ 電流の大きさが小さく、抵抗の大きさが大きかったから。\n",
      "709 　　　 × 抵抗が大きく、電流が流れにくかったから。\n",
      "729 誤採点 ◯ 直列につないだことによって2つの電熱線の低抗が合わさって電流が流れにくくなったから。\n",
      "745 　　　 × 抵抗が大きくなり電流が届かなくなっているから。\n",
      "766 　　　 × 抵抗が1つのときよりも大きく電流が流れにくいから。\n",
      "860 　　　 × 1つの時よりも抵抗が大きくなるので電流の大きさは大きくなる。\n",
      "896 誤採点 × 電熱線が1つのときよりも、抵抗が大きく、電流が流れにくくなるため。\n",
      "1052 　　　 × 電熱線が増えたことで抵抗が大きくなり電流が流れにくくなったから\n",
      "1189 誤採点 ◯ 低抗が大きくなって電流が流れにくくなった、\n",
      "1325 誤採点 ◯ 電流が小さく抵抗が大きいから。\n",
      "1343 　　　 × 電圧を大きくしても、電熱線が2つだと、抵抗が大きくなり、電流が流れにくくなるから。\n",
      "1414 　　　 × 空気抵抗が強く、電流が流れにくいため。\n",
      "1515 　　　 × 抵抗が大きくなって、電流の向きが変わったから。\n",
      "1581 誤採点 ◯ 電熱線を直列につなぎ数を増やすと、低抗が強くなり電流が小さくなるから。\n",
      "1615 　　　 × 抵抗が1つ増えたため電流が流れにくくなったから。\n",
      "1651 　　　 × 発泡ポリスチレンがはいってくると、抵抗が大きくなって、電流が流れにくくなるから。\n",
      "1793 　　　 × 抵抗が大きく、電流が上がりにくかったから。\n",
      "1815 　　　 ◯ 1つの時にはなかった抵抗ができ電流が小さくなったから\n",
      "1842 　　　 ◯ 抵抗が多く、電流が流れにくかったため\n",
      "1854 　　　 ◯ 電熱線が1つのときよりも抵抗がまして電流が流れにくくなったため。\n",
      "1869 誤採点 × 電熱線を直列つなぎにしたので、回路全体の抵抗が大きくなり、電流が流れにくくなったから。\n",
      "1897 誤採点 ◯ 電熱線が2個なので、低抗も2倍になり、電流の大きさが半分になったから。\n",
      "誤採点検出回数： 40 11\n",
      "検出回数： 12\n",
      "==================================================\n",
      "64 　　　 × 抵抗力が上がり電流が流れにくくなったため。\n",
      "92 　　　 × 抵抗が2倍になり、電流が電熱線が1つのときよりも流れにくくなるから。\n",
      "129 　　　 × 水だけの時よりも抵抗が強くなり電流が小さくなってしまうから。\n",
      "180 　　　 × 直列に抵抗である電熱線をつなげると、より抵抗が大きくなり、電流が流れにくくなるため。\n",
      "197 　　　 × 電熱線が1つのときより抵抗が小さいため、電流の大きさが小さくなったから。\n",
      "254 　　　 × 流れる電流は1つのときと2つのときでかわらないが、抵抗は1つのときよりも2つのほうが2倍になり、電流が流れにくくなったから。\n",
      "328 　　　 ◯ 電流が流れにくくなったため、抵抗が大きくなり、温度が上がりにくいから。\n",
      "445 　　　 × 抵抗が大きすぎて、電流があまり流れなかったから。\n",
      "602 　　　 × 電熱線の数が増えて抵抗が大きくなり、電流が流れにくくなるから。\n",
      "631 　　　 × 抵抗が大きくなり電流が効率よく流れなかったため。\n",
      "768 　　　 × 電熱線が増える分抵抗も大きくなって電流が伝わりにくくなるから。\n",
      "778 　　　 × 電熱線を直列につないだことで抵抗が強くなり、電流が流れにくくなったから。\n",
      "830 　　　 × 電圧の大きさが小さく、抵抗が大きいので、電流が流れにくいから。\n",
      "892 　　　 × 抵抗が大きくなったため、電流があまり流れず、水の上昇温度が低かった。\n",
      "913 　　　 × 抵抗により、電流の大きさが小さくなったから。\n",
      "1054 　　　 × 直列回路にしたため、電熱線をふやしたことで実験よりも低抗が大きくなり、電流の大きさが小さくなったから。\n",
      "1173 誤採点 × 抵抗が大きくなると、電流が流れにくくなるから\n",
      "1176 　　　 × 直列電路で抵抗が大きく電流が流れにくいから。\n",
      "1256 　　　 × 抵抗が大きくなり電流が流れにくくなったため。\n",
      "1385 　　　 ◯ 電熱線1つの抵抗が5Ωだから、２つあるので10Ωの抵抗がある、電圧は8.0Vなので電流は0.8Aになる、この値は電熱線１つのときよりも低いため、水の上昇温度は低い。\n",
      "1449 　　　 × 抵抗が大きく、電流が流れにくかったから。\n",
      "1462 　　　 ◯ 抵抗が2つの電熱線の和になり電流が流れにくくなるから\n",
      "1470 　　　 × 抵抗が大きくなり、電流が流れにくくなったため。\n",
      "1475 　　　 ◯ 抵抗が高くなり、電流が流れにくくなったから。\n",
      "1782 　　　 ◯ 抵抗する物が増えたら電流も弱まるから。\n",
      "1857 　　　 × 直列にしたことで回路全体の抵抗が大きくなり電流が流れにくかったから。\n",
      "1927 　　　 × 電熱線が1つのときよりも2つのときの方が電流が大きくなるため電圧が小さくなったから。\n",
      "1957 　　　 × 2つの電熱線にかかる電流は等しいが、回路全体の抵抗が大きくなって、1つ1つの電熱線にかかる電圧がちいさくなって、それにより電熱線にかかる電力が小さくなったため。\n",
      "誤採点検出回数： 28 1\n",
      "検出回数： 11\n",
      "==================================================\n",
      "99 　　　 × 2つの電熱線を直列につなぐと、電圧8.0Vのとき、抵抗は10Ωになり、電流は0.2Aになってしまうから。\n",
      "172 誤採点 ◯ 低抗の大きさが大きくなり、電流が流れにくくなるから。\n",
      "323 　　　 × 2つにしたことにより、抵抗が強くなり、電流が流れにくくなったから。\n",
      "403 　　　 × 抵抗が大きく、電流が流れにくかったから。\n",
      "520 　　　 ◯ 抵抗が大きくなったので電流が小さくなり水に温度が伝わらなかったから\n",
      "559 　　　 ◯ 電熱線が2つになったので抵抗が強まり、電流があまり流れないから\n",
      "634 　　　 × 抵抗が増え、電流が通りずらくなったから。\n",
      "757 　　　 ◯ 抵抗が大きく、電流が実験のときよりも流れなかったから。\n",
      "765 誤採点 × 電熱線の数がふえると、抵抗も大きくなるので電流が流れにくくなるから。\n",
      "916 　　　 × 抵抗が大きくなるほど電流が流れにくくなるから\n",
      "947 　　　 × 抵抗が大きくなり、電流があまり流れなかったから\n",
      "970 　　　 × 実験より抵抗が大きくなり、電流が流れにくくなったため。\n",
      "1035 　　　 ◯ 電流が弱く抵抗が大きかったから\n",
      "1157 　　　 × 低抗がふえたため、電流が通りにくくなったから。\n",
      "1231 　　　 × 電熱線の抵抗が増え、電流が流れにくくなるから。\n",
      "1278 　　　 × 電熱線を増やしたことにより、抵抗が大きくなり、電流が流れにくくなった。\n",
      "1297 　　　 × 電熱線が1つのときよりも、抵抗が大きくなったので、電流が流れにくくなったから。\n",
      "1333 　　　 ◯ 抵抗の数が増えたため電流が流れにくくなった\n",
      "1415 　　　 ◯ 抵抗が大きくて、電流が少なかったから。\n",
      "1494 　　　 × 発砲ポリスチレンは電流をあまり通さないため、抵抗が大きくなり、流れる電流が小さくなるから。\n",
      "1659 　　　 × 抵抗が増えたせいで水に送られる電流が減ったから。\n",
      "1806 　　　 × 直列につないだことにより、抵抗が大きくなり、電流が小さくなったため\n",
      "1811 　　　 × 抵抗が大きくなり、必要な電流が大きくなったから。\n",
      "1935 　　　 × 抵抗が大きくなり電量が小さくなったため\n",
      "1976 　　　 ◯ 抵抗が多く電流があんまり流れなかったため。\n",
      "2011 誤採点 ◯ 低抗が大きくなったことで、電流の大きさが小さくなったから。\n",
      "誤採点検出回数： 26 3\n",
      "検出回数： 10\n",
      "==================================================\n",
      "224 　　　 ◯ 抵抗が大きくなったため、電流をよくせいする量が大きくなったから。\n",
      "250 　　　 × 抵抗の大きさが大きくなり、電流が大きくなったから。\n",
      "293 　　　 × 抵抗が2倍になることで、電流がもっと流れにくくなったため\n",
      "437 　　　 × 抵抗の数が増えて、電流が流れにくくなったから。\n",
      "458 誤採点 × 抵抗が大きくなると電流が流れにくくなる。\n",
      "710 　　　 ◯ もう一つの方と比べ電流が弱く抵抗かつよかったから。\n",
      "711 　　　 × 抵抗が大きくなることで電流が流れにくくなる\n",
      "848 　　　 ◯ 抵抗が大きく、電流がよわいから。\n",
      "963 　　　 × 電熱線の低抗の大きさが2倍になったから、より電流が流れにくくなったから\n",
      "1091 　　　 × 1つのときより2つのときのほうが抵抗が大きく、電流を多くつかうから。\n",
      "1166 誤採点 × 直列につないだので回路全体の抵抗が大きくなり、電流が流れにくくなったから。\n",
      "1167 　　　 ◯ 抵抗が大きく、電流がさまたげられたから\n",
      "1327 　　　 × 電熱線を直列につなぐと抵抗が大きくなり電流はかわらないが電圧が小さくなるから。\n",
      "1339 　　　 × 低抗が大きくなり、電流の大きさが小さくなったから。\n",
      "1488 　　　 × 抵抗が大きく、電熱線が2つのときより電流が流れなかったから。\n",
      "1534 　　　 × 抵抗の量が多くなっているため、電流が流れにくくなっているから。\n",
      "1567 　　　 × 抵抗がふえて電流が逃げてしまうから。\n",
      "1652 　　　 ◯ 抵抗が2つになったため電流の大きさが半分になってしまったから。\n",
      "1719 　　　 × 抵抗する力が弱かったから電流があんまり流れなかった\n",
      "1816 　　　 × 電熱線が2つになり、抵抗が大きくなったことで、電流が流れにくくなったから。\n",
      "1837 　　　 × 電熱線が2つのほうが抵抗がちいさく、電流が流れやすいから。\n",
      "1862 　　　 ◯ 流れる電流が抵抗の力が大きくなったことにより小さくなったから\n",
      "1918 　　　 × 抵抗が大きくなり電流が小さくなったため、熱が空気中に逃げてしまったから。\n",
      "2009 　　　 ◯ 抵抗が多いほうが電流がながれにくいから。\n",
      "誤採点検出回数： 24 2\n",
      "検出回数： 9\n",
      "==================================================\n",
      "18 　　　 × 抵抗が流れにくいため電流が流れなかったから。\n",
      "63 　　　 ◯ 抵抗が、大きくなり、電流が流れにくくなったため。\n",
      "67 　　　 × 電熱線が１つのときよりも大きな抵抗が加わり、電流が流れにくいから。\n",
      "182 　　　 × 抵抗により流れる電流が少なくなったため。\n",
      "226 　　　 × 1つの電熱線の抵抗が大きく、電流が流れなかったため。\n",
      "233 　　　 ◯ 電熱線が増えたため全体の抵抗が大きくなり電流が流れにくくなった。\n",
      "345 　　　 × 抵抗が高く、電流が弱かったため。\n",
      "390 　　　 × 1つのときよりも低抗が大きくなり電流が伝わりにくくなったから。\n",
      "427 　　　 ◯ 抵抗が1つより2つの方が大きく、電流が流れずらいから。\n",
      "650 　　　 ◯ 電熱線の抵抗が実験よりも大きく電流が流れにくかったから。\n",
      "680 誤採点 × 電熱線が増えたため抵抗が大きくなり、電流が流れにくくなったから\n",
      "817 　　　 × 直列つなぎにすると、全体の抵抗の大きさは5＋5＝10で、10Ω、電流は0.8Aとなり、実験のときよりも発熱量が小さくなるから。\n",
      "827 　　　 ◯ 抵抗がつよく、電流があまり、ながれなかったため、\n",
      "1051 　　　 ◯ 電熱線の抵抗が大きいから、電流があまり流れなかったから。\n",
      "1233 　　　 × 抵抗が2つになったため、電流の大きさが小さくなったから。\n",
      "1445 　　　 × 抵抗の大きさに比べて流れる電流の大きさが小さかったから。\n",
      "1508 　　　 × 2つに増えてしまったため、抵抗が分かれて、電流も半分になってしまった。\n",
      "1657 　　　 ◯ 抵抗が大きすぎたため、電流が流れずらくなっていたから。\n",
      "1712 　　　 ◯ 抵抗が1つのときよりふえ、電流がよわまるから。\n",
      "1745 　　　 ◯ 直列につなぐと、2つの抵抗の大きさを足した大きさになって、抵抗が1つのときよりも電流が流れにくくなるから。\n",
      "1881 　　　 ◯ 直列回路でつないだことで抵抗が増え、電流が流れにくくなったから。\n",
      "1995 　　　 × 電源電圧が実験よりも大きく、抵抗も大きかったので、電流が流れにくくなってしまったから。\n",
      "誤採点検出回数： 22 1\n",
      "検出回数： 8\n",
      "==================================================\n",
      "3 　　　 × 抗抗の値が増え、電流の値が減ったから。\n",
      "102 　　　 × 電熱線の抵抗が同じでも、電圧の大きさが小さくなると、電流の大きさも小さくなるから、水がえる熱量は少なくなるから。\n",
      "114 　　　 ◯ 直列回路は、電熱線が1つのときと比べると、抵抗が2倍になり、電流が1/2倍になるため、水の上昇温度は低くなるから。\n",
      "147 　　　 × 水への抵抗が大きく、電流が小さくなったから。\n",
      "309 　　　 ◯ 2つに増えたことでその分、抵抗も大きくなったが電圧はそのままだったので電流が流れにくくなってしまったから。\n",
      "466 　　　 ◯ 電熱線が1つのときよりも、抵抗が大きくなり、電流が流れにくくなったから\n",
      "541 　　　 × 抵抗があるとより電流の大きさが小さくなるから。\n",
      "706 　　　 ◯ 抵抗が大きくて、電流の大きさが電熱線が一つのときよりも小さかったから。\n",
      "717 　　　 × 電流が少なく抵抗が大きかったから\n",
      "836 　　　 × 電熱線が1つのときよりも、抵抗が大きくなったため、電流が流れにくくなったから。\n",
      "859 誤採点 ◯ 低抗が大きくなり1つのときより電流の量が少なくなったから。\n",
      "930 　　　 ◯ 2つも、同じ回路でやってしまうと、抵抗がものすごく大きくなり電流は弱くなってしまい、2つ入れると温度が低い。\n",
      "984 　　　 × 同じ抵抗の電熱線を2つ使うことで、電圧を大きくしても抵抗が大きければ電圧は小さくなるので、水の上昇温度は低くなる。\n",
      "1077 　　　 ◯ 電熱線が増えたことで抵抗が大きくなって電流が弱くなるから\n",
      "1132 　　　 × 抵抗が多くなって電流が通りづらくなったから\n",
      "1295 誤採点 × 直列につないだため抵抗は大きくなり、電流が流れにくくなったから。\n",
      "1306 　　　 × 電熱線を1つふやしたことによって、低抗が大きくなったので、同じ量の電流が通らなくなったから。\n",
      "1307 　　　 × 抵抗が大きくなったので、電流の流れがおそくなったから。\n",
      "1342 　　　 × 抵抗をもう1つ増やしたことによって、発生する電流の量が少なかったから。\n",
      "1406 　　　 ◯ 直列につなぐと抵抗の大きさは、抵抗の合計になり、電流が電熱線が1つのときより流れにくいから。\n",
      "1474 　　　 × 電流が抵抗が大きくなったことで、熱が流れにくくなったから。\n",
      "1654 　　　 ◯ 抵抗がふえたため、電流の流れもよわくなったから\n",
      "1875 　　　 × 電熱線をもう1つ用意したことによって抵抗が生まれ電流が流れにくくなってしまったから。\n",
      "1980 　　　 ◯ 電熱線の抵抗が増え、電流が流れにくくなったから。\n",
      "2012 　　　 × 抵抗が弱くなって、電流の強さが強くなった。\n",
      "誤採点検出回数： 25 2\n",
      "検出回数： 7\n",
      "==================================================\n",
      "153 　　　 ◯ 電熱線を直列につないだことにより、抵抗が大きくなり、流れる電流の量がへったため、水の上昇温度は1つの時より低くなった。\n",
      "235 　　　 × 抵抗を1つ増やしたことで、流れている電流の大きさが小さくなり、温度があまり上がらなかった。\n",
      "240 　　　 ◯ 電熱線を直列でつなぐと抵抗が増えるため電流が流れにくくなり、水の上昇温度が下がった\n",
      "257 　　　 ◯ 抵抗が2倍になり、同じ電圧で流れる電流の量が減ったので水の温度を上げる電熱線の発熱量も少なくなったから。\n",
      "306 　　　 ◯ 抵抗が同じ大きさの電熱線を直列につなぐと、回路全体の抵抗の大きさが1つの電熱線の抵抗より大きくなって、流れる電流の大きさがちいさくなるため。\n",
      "395 　　　 ◯ 抵抗が電熱線1本分増えるため、その分電流が流れにくくなるから。\n",
      "404 　　　 × 電熱線を2つにすると抵抗が高くなり、電流が少さくなるから\n",
      "419 　　　 ◯ 2つの電熱線を使ったことにより、抵抗が大きくなり電流が小さくなったため、水の上昇温度は低くなった。\n",
      "442 　　　 ◯ 電熱線が1つのときよりも、抵抗が大きくなったため、電流が流れにくかったから。\n",
      "579 　　　 × 電圧は高くしたけど、抵抗が大きく、電流が流れにくかったため。\n",
      "585 　　　 × 抵抗が高くなって電流が流れにくくなったから\n",
      "816 　　　 × 電熱線が2個に増えたため、抵抗が増え電流が流れにくくなった。\n",
      "927 　　　 × 抵抗が2つになり電流が流れづらくなったから\n",
      "934 　　　 ◯ 直列つなぎにしたため抵抗が2つ分となり、流れる電流が小さくなったため。\n",
      "1086 　　　 ◯ 回路全体の抵抗の大きさが大きくなり、電流が電熱線が1つのときよりも流れにくくなったから。\n",
      "1165 　　　 ◯ 抵抗が大きく、電流が流れにくかったから。\n",
      "1232 　　　 ◯ 抵抗が大きくなって電流がより流れにくくなったから。\n",
      "1264 　　　 × 抵抗が大きくなってしまい、電流が大きくならなかったから。\n",
      "1371 　　　 × 抵抗があるため電流が流れにくい。\n",
      "1442 　　　 ◯ 電熱線を直列につないだことで抵抗が大きくなり、電流が流れにくくなったから。\n",
      "1450 　　　 × 電熱線が1つのときと2つのときでは抵抗と電流の大きさが同じ時、2つのときのほうが流れる電流の大きさが小さいから。\n",
      "1519 　　　 × 電熱線の抵抗と電流が半分になって小さくなってしまうから。\n",
      "1603 　　　 ◯ 電熱線を直列につないだことで、回路全体の抵抗が大きくなり、電流が流れにくくなったから。\n",
      "1608 　　　 ◯ 抵抗が大きくなり電流が十分に流れてなかったから。\n",
      "1628 　　　 ◯ 抵抗か強くて電流が流れづらかったから\n",
      "1707 　　　 × 2つの電熱線の抵抗の分だけ、電流が流れてしまい、水に流れる電流が少なくなってしまうから。\n",
      "1732 　　　 ◯ 抵抗は、直列につなぐほうが大きくなり、電流が流れにくくなったため。\n",
      "1907 　　　 ◯ もとの電流が抵抗の数がふええたから、伝わる電流の量が減ってしまったから\n",
      "誤採点検出回数： 28 0\n",
      "検出回数： 6\n",
      "==================================================\n",
      "19 　　　 × 抵抗を2つにしたことで電流の大きさが半分になったから。\n",
      "96 　　　 ◯ 抵抗力が増えて電流が流れにくくなったため。\n",
      "175 誤採点 × 電熱線の量が増えると、抵抗が大きくなり電流が流れにくくなったから\n",
      "249 　　　 × 電圧が大きくなった分低抗が増え電流が流れにくくなった\n",
      "278 　　　 × 電熱線の抵抗が強く、電流があまり流れなかったため。\n",
      "673 　　　 × 電熱線が増えたことにより、抵抗が高くなって電流が流れにくくなったから。\n",
      "719 　　　 × 電流が2つの電熱線にながれて一つに流れる電流の大きさが小さくなり抵抗が小さくなるから。\n",
      "785 　　　 × 抵抗は電流に比例するため、抵抗の大きさが大きくなると、全体の電圧は小さくなるから\n",
      "877 　　　 × 電熱線が増え抵抗が大きくなり電流が大きくなったため\n",
      "956 　　　 × 電熱線1本の方が抵抗は少ないが、電熱線2本の方が抵抗は大きいが、電流が大きいから。\n",
      "1030 　　　 ◯ 電熱線を直列に2つつないだため、抵抗が大きくなり、電流が流れにくくなったため。\n",
      "1078 　　　 × 抵抗が大きく電流をとおさないから。\n",
      "1087 　　　 ◯ 抵抗が大きくなったことにより、電流が流れにくかった。\n",
      "1103 　　　 × 電熱線が2つになったため、抵抗が小さくなり、電流が小さくなった。\n",
      "1104 　　　 × 抵抗は小さくなり、流れる電流が多くなったから。\n",
      "1206 　　　 × 電熱線の抵抗が大きく実験よりも電流が流れなかったから。\n",
      "1212 　　　 ◯ 2つの電熱線を直列につないだことから抵抗が大きくなったため、電流が流れにくくなったから。\n",
      "1224 　　　 ◯ 抵抗が強くて電流が流れにくかった\n",
      "1241 　　　 ◯ 電流は抵抗が大きいほど流れにくくなるから。\n",
      "1267 　　　 × 抵抗が多くなり電流が流れにくくなった。\n",
      "1403 　　　 × 抵抗の強さで電流が弱くなってしまったから。\n",
      "1443 　　　 ◯ 電熱線が多くなると、全体の抵抗も大きくなり、電流が流れにくくなるから。\n",
      "1487 　　　 ◯ 電熱線が2つになった事により、抵抗が大きくなり電流がちいさくなった\n",
      "1604 　　　 ◯ 抵抗が大きくなり、1つの電熱線にかかる電流の大きさが小さくなったから\n",
      "1634 　　　 ◯ 電熱線の抵抗値が高くなり、電流が流れにくくなったから。\n",
      "1714 　　　 ◯ 抵抗が大きく、電流が流れにくかったから。\n",
      "1873 　　　 × 電圧の大きさを上げて抵抗を変えなかった場合電流の大きさは小さくなるから。\n",
      "1883 　　　 × 抵抗が2つになったことにより、並列回路になり、流れる電流が小さくなったから。\n",
      "2006 　　　 × 抵抗が弱くなり、電流の流れる量がふえたから。\n",
      "誤採点検出回数： 29 1\n",
      "検出回数： 5\n",
      "==================================================\n",
      "41 　　　 × 抵抗が小さく、電流が流れにくかったため。\n",
      "80 　　　 × 電熱線が増えることで抵抗しきれなくなり電流の流れが少なくなってしまうから\n",
      "120 誤採点 ◯ 電熱線を直例でつなぐと全体の低抗が大きくなり電流が流れにくくなるから。\n",
      "151 　　　 ◯ 電圧を変えずに回路全体の抵抗を大きくすると、電流は小さくなってしまうから。\n",
      "217 　　　 ◯ 抵抗がふえ、電流が流れずらくなったため。\n",
      "291 　　　 ◯ 全体に流れている電圧は一つの時と二つの時は等しいので抵抗の値が2倍になると流れる電流は一つの時の1/2になる\n",
      "304 　　　 × 抗抗が大きく、電流が流れにくかったから。\n",
      "515 　　　 ◯ 抵抗が大きくなり、水に流れる電流が少なくなったから。\n",
      "524 　　　 ◯ 電熱線を2つにすると、1つの電熱線のときより全体の抵抗が大きくなり、1つの電熱線にかかる電流の大きさが小さくなるから。\n",
      "614 　　　 ◯ 抵抗が大きくなったため、電流の流れがすくなくなったから。\n",
      "656 　　　 ◯ 電熱線が増えると、抵抗が大きくなるので、電流が流れにくくなるから。\n",
      "663 　　　 × 抵抗が小さくなって電流が流れにくくなるから。\n",
      "699 　　　 ◯ 直列の方が抵抗が大きく、電流があまり流れないから\n",
      "713 　　　 × 1つの電熱線のときよりも抵抗が大きかったため、電流が、流れにくくなった。\n",
      "801 　　　 × 電源電圧が低くかったせいとよけいに抵抗がふえて電流が低くかったから\n",
      "834 　　　 ◯ 抵抗をふやすと、電流の大きさは小さくなるから。\n",
      "850 　　　 × 抵抗が小さく電流が流れずらいから。\n",
      "880 　　　 ◯ 抵抗が上がってしまい電流が流れにくくなったから\n",
      "925 　　　 × 抵抗する力も弱まり電流も弱くなったから\n",
      "940 　　　 × 抵抗がつよく電流がうまくとうらないため\n",
      "975 　　　 ◯ 抵抗が増えると電流は下がるから\n",
      "980 　　　 × 電熱線が1つのときよりも水の上昇温度が低かったのは、低抗が小さく、電熱線に流れる電流の大きさが大きいから。\n",
      "989 　　　 × 電流はおおく流れるが抵抗がおおきいため温度は低くなった\n",
      "1048 　　　 × 電熱線の1つあたりの抵抗が小さく、電流の大きさが大きくなるため。\n",
      "1050 　　　 × 2つの電熱線を直列につないだので、1つの時と抵抗は同じだった。だが、流れる電流は大きいため水の上昇温度が低くなった\n",
      "1107 　　　 ◯ 抵抗の力が強く、電流が流れにくかったため。\n",
      "1191 　　　 × 2つの抵抗が合わさり、電熱線に加わる電流の大きさを小さくしたから。\n",
      "1197 　　　 ◯ 直列につないだため、抵抗の大きさが大きくなり、電流が流れにくくなったから。\n",
      "1367 　　　 ◯ 抵抗が大きくなったので、電流を流しにくくなり、熱が伝わりにくくなった。\n",
      "1398 　　　 × 抵抗が多く、電流が流れにくかった\n",
      "1461 　　　 ◯ 抵抗が強いと、電流が通りづらくなる。\n",
      "1477 　　　 × 電流の大きさが小さいのと、抵抗が大きいから。\n",
      "1545 　　　 × 電熱線を2つにすることで電流は上昇するがその分抵抗も上昇するため、抵抗が上回り、結果的に温度が電熱線が1つの時より下がった。\n",
      "1570 　　　 ◯ 抵抗を大きくしたため電流が小さく水があまりあたたまらなかったから。\n",
      "1698 　　　 ◯ 電熱線が増え、抵抗が大きくなって、電流が小さくなり、電流が小さいほど、水の上昇温度は低いから。\n",
      "1723 　　　 × 電流が大きいほど、抵抗が大きくなり、さまたげるため。\n",
      "1751 　　　 ◯ 抵抗が大きく電流がなかなか流れず水温があまり上昇しない\n",
      "1768 　　　 ◯ 直列につないだことで抵抗が大きくなり電流がよわくなったから。\n",
      "1785 誤採点 ◯ 実験のときと比べて、低抗が大きく、流れる電流が低いから。\n",
      "1821 　　　 × 1つの時は電流がそこにしか流れず抵抗もあれだけど、2つになると電流が別かれて流れるのが半分になってしまうから。\n",
      "誤採点検出回数： 40 2\n",
      "検出回数： 4\n",
      "==================================================\n",
      "2 　　　 ◯ 電熱線を増やすことにより抵抗が大きくなり、電熱線へ行く電流の大きさも1/2になるから。\n",
      "157 　　　 × 電熱線1つのときよりも、抵抗が低くなり電流が流れにくくなったから。\n",
      "191 　　　 × 2つになる分、抵抗も2倍になり、電流を2倍になるから。\n",
      "204 　　　 ◯ 抵抗が大きく、電流が流れにくかったから。\n",
      "264 　　　 ◯ 直列つなぎのため、抵抗が大きくなり電流が流れにくくなるから。\n",
      "324 　　　 ◯ 抵抗は直列つなぎなので大きくなって電流は小さくなるから。\n",
      "352 誤採点 ◯ 回路の低抗が大きくなって、電熱線が1つのときよりも電流が流れにくくなってしまったため。\n",
      "413 　　　 × 抵抗を直列に2個つないだことにより、電流にかかる抵抗が大きくなり、発熱量が少なくなるから。\n",
      "439 　　　 × 1つのときより抵抗が2倍あるので電流の大きさが2つに分かれてしまったから\n",
      "496 　　　 ◯ 電圧の大きさをかえたので抵抗が大きくなり電流が流れにくくなったから。\n",
      "498 　　　 × 抵抗が強く、電流がうまく届いていないから。\n",
      "509 　　　 ◯ 電熱線の数が増えたので抵抗が大きくなり電流の大きさが小さくなったから。\n",
      "514 　　　 × 抵抗の数が増えたことで、電流が流れにくくなったから。\n",
      "570 　　　 ◯ 発熱線を2つ直列につないだことによって、回路全体の抵抗が大きくなり、電流が流れにくくなったため。\n",
      "603 　　　 × 抵抗が増えたことによって電流が多くさえぎられたから。\n",
      "645 　　　 × 流れる電流に対する抵抗が大きくなり熱が発生しにくいため。\n",
      "736 　　　 ◯ 2つの電熱線を使うこによって、抵抗が大きくなり、電流が流れにくく、温度が上がりにくかった。\n",
      "864 　　　 ◯ 抵抗が同じ電熱線を使うと抵抗が大きく電流が流れにくくなるため\n",
      "895 　　　 ◯ 電熱線が1つのときよりも全体抵抗が大きいので、より大きい電流が流れなかったから。\n",
      "982 　　　 ◯ 電熱線が増えたことによって抵抗が増えたため電流が実験よりも流れなかったので温度が低くなった。\n",
      "988 　　　 ◯ 抵抗が大きく電流がちゃんと流れなかったから\n",
      "1033 　　　 ◯ 抵抗が大きく電流がおさえられたから。\n",
      "1036 　　　 × 抵抗が小さい時ほと電流が通りやすいから。\n",
      "1083 　　　 ◯ 抵抗の値が高かったため、電流があまり流れなかったから。\n",
      "1084 　　　 × 電流が大きいほど抵抗が大きいため温度があがりにくい。\n",
      "1141 　　　 × 抵抗の数が多いため、電流が小さくなるため。\n",
      "1143 　　　 × 抵抗が小さく、電流も小さいから\n",
      "1147 　　　 × 抵抗が弱くて電流が流れにくかったから。\n",
      "1252 　　　 × 抵抗を直列回路でつないた場合2つの抵抗の和より小さくなり、電流も同じ大きさでしか流れないから。\n",
      "1323 　　　 × 抵抗が大きいと同じ大きさの電流や電圧を流しても得られる熱量は小さくなるため。\n",
      "1365 　　　 × 抵抗の大きさが小さくなり、電流が流れにくくなったため\n",
      "1369 　　　 ◯ 抵抗が大きく電流がたくさん流れなかった。\n",
      "1386 　　　 × 電熱線が1つのときよりも、2つのようが、電熱線1つにかかる抵抗が弱くなるので、電流の大きさも小さくなるから。\n",
      "1391 　　　 ◯ 電熱線が2つあるため、抵抗が電熱線1つのときと比べて大きいので、電流が流れにくい\n",
      "1409 　　　 × 抵抗が増えたことにより、電流をさえぎるのも大きくなったから。\n",
      "1448 　　　 ◯ 抵抗が増えたため、伝わる電流の量が少なくなってしまったため、水の上昇温度は低かった、\n",
      "1454 　　　 ◯ 抵抗が大きかったため、電流の流れがおさえられてしまったから。\n",
      "1497 　　　 ◯ 電熱線が1つのときよりも抵抗が強く電流が流れにくくなったため、水に熱が伝わりにくくなったから。\n",
      "1510 　　　 ◯ 抵抗が増えたことによって電流がさらに流れにくくなったから\n",
      "1561 誤採点 ◯ 直列回では電熱線が2つつながると低抗が大きくなるため流れる電流も少なくなるため。\n",
      "1595 　　　 × 抵抗が大きくなって電流がさえぎられたから\n",
      "1597 　　　 × 抵抗が増えて流れる電流がぶんさんしてしまったから。\n",
      "1607 　　　 ◯ 抵抗が大きくなったので、電流が小さくなり、あたたかさが伝わりにくくなったため。\n",
      "1708 　　　 ◯ 二つになると抵抗の大きさも大きくなるため電流が流れにくくなり水の上昇温度が低くなった。\n",
      "1717 　　　 ◯ 電熱線を直列につなぐと抵抗が大きくなり、同じ電圧でも電流が小さくなるから。\n",
      "1724 　　　 ◯ 水の中の抵抗が大きくなったため電流が通りにくなったから。\n",
      "1796 　　　 × 抵抗が大きく、電流の流れがおそくなったから。\n",
      "1839 　　　 × 電熱線が並列につながれているため、電流が1つの電熱線より小さくなり、抵抗も半分の大きさになったから\n",
      "1895 　　　 × 抵抗が大きくなったから電流が\n",
      "1979 　　　 ◯ 電熱線が2つにふえたことによって、抵抗の大きさがふえすぎてしまったので、電流がながれにくくなってしまったので、温度がひくい。\n",
      "1987 　　　 ◯ 抵抗が高まり電流が流れにくくなったから。\n",
      "誤採点検出回数： 51 2\n",
      "検出回数： 3\n",
      "==================================================\n",
      "31 　　　 × 電熱線が1つのときよりも、抵抗が大きく、電流はかわらないため、上昇温度も低くなる。\n",
      "178 　　　 ◯ 電熱線が1つのときよりも、2つのときのほうが抵抗が大きくなり、電流が流れずらくなるから。\n",
      "299 　　　 ◯ 抵抗が強く電流が伝わりにくかったため水の上昇温度は低くなった、\n",
      "312 　　　 × 電熱線を2つに増やしたことで電流が大きくなってそれにともなって抵抗が小さくなったため。\n",
      "317 　　　 × 直列の路になったことで抵抗が小さくなり、電流が弱くなったため\n",
      "336 　　　 ◯ 2つにすると抵抗が大きくなり、電流が流れにくくなるから\n",
      "360 　　　 × 1つの抵抗に加わる電流が小さくなったから。\n",
      "383 　　　 ◯ 抵抗が大きくなり、電流が流れにくくなったため\n",
      "388 　　　 × 低抗が大きく、電流が流れにくかったため。\n",
      "453 　　　 ◯ 抵抗が大きくなり、電流が小さくなるから。\n",
      "457 　　　 ◯ 抵抗が大きくなり電流が流れにくくなってしまったため。\n",
      "479 　　　 × 抵抗がよわるり、電流がながれやすくなった。\n",
      "547 　　　 ◯ 電熱線が増えることにより、抵抗が増し、電流が流れにくくなったから。\n",
      "573 　　　 × 抵抗を直列につなぐと、抵抗の大きさの和になり、1つの時に比べて、抵抗が大きく、電流の大きさもかわらないので水の上昇温度は低くなった。\n",
      "586 　　　 ◯ 抵抗は2倍になるが、電流は減ってしまうから。\n",
      "590 　　　 ◯ 抵抗が強くなり、電流も弱くなったから。\n",
      "620 　　　 ◯ 低抗が大きくなり、流れる電流が小さくなったから\n",
      "623 　　　 ◯ 全体の抵抗の大きさが、電熱線が1つのときの抵抗の大きさの2倍になったため、その文電流が流れにくくなったから。\n",
      "666 　　　 ◯ 抵抗が2倍になり、１つのときよりも電流が流れにくくなったから。\n",
      "784 　　　 ◯ 直列つなぎは、抵抗が大きく、電流が流れにくいため。\n",
      "808 　　　 ◯ 抵抗が同じのを1つ増やしたということで抵抗が2倍になり電流が小さくなったため、上昇温度は低くなった。\n",
      "810 　　　 ◯ 抵抗が大きくなって電流が流れずらくなったから\n",
      "831 　　　 ◯ 抵抗が大きくなったため、電流が流れずらくなったから。\n",
      "835 　　　 ◯ 抵抗の大きさが電熱線1この時よりも大きいので電流の大きさは小さくなる\n",
      "867 　　　 ◯ 電熱線を2つにしたことで、抵抗が増え、電流の大きさが少なくなったから。\n",
      "881 　　　 × 抵抗ができ、電流が流れにくくなったから。\n",
      "885 　　　 ◯ 電熱線が増えて抵抗が大きくなり、電流が流れにくくなったから。\n",
      "901 　　　 ◯ 抵抗が大きくなり、電熱線に電流が流れにくくなったから。\n",
      "905 　　　 ◯ 電熱線が1つのときよりも抗抵が大きくなり、電流が流れにくくなったから。\n",
      "939 　　　 ◯ 抵抗が大きくなり、電流があまり流れなかったから。\n",
      "946 誤採点 ◯ 直線つなぎだと、低抗の大きさがより大きくなって電流の大きさが小さくなるから。\n",
      "1149 　　　 × 電熱線を2つ直列につないだから抵抗が高くなり電流が弱くなった。\n",
      "1182 　　　 ◯ 電圧は変わらないのに抵抗が大きくなったため、電流の値が小さくなり、電熱線の発熱量も小さくなったから。\n",
      "1217 　　　 ◯ 2ついれることで抵抗が増え、電流が流れにくくなったから。\n",
      "1254 　　　 × 抵抗が2つあるので図1のときより電流は流れにくいから\n",
      "1281 　　　 × 直列回路は抵抗を2つたした1/2のていこうになるので電流がながれやすくなってしまった\n",
      "1322 　　　 × 電流を流したときに、電流の力に抵抗する力が大きくなったため、なかなか温度が上がらなかったから。\n",
      "1521 　　　 ◯ 抵抗の大きさが同じ電熱線をつなげると抵抗の大きさが2倍になるため、電圧の大きさが変わらない以上、電流の大きさは小さくなるから。\n",
      "1523 　　　 × 抵抗が大きくなって、電流による水温の上昇が小さくなった。\n",
      "1591 　　　 ◯ 抵抗が大きくなったため、あまり電流が流れなったから。\n",
      "1611 　　　 × 電熱線をもう1つつなげたことにより、電流は強くなったが、低抗も強くなったので、より、伝わりにくくなったため。\n",
      "1629 　　　 ◯ 二つの電熱線により、抵抗が強くなり電流が流れにくくなる\n",
      "1645 　　　 ◯ 電圧は変わらなくて、抵抗が大きくなると、電流の大きさは小さくなるから\n",
      "1685 　　　 ◯ 1つの時よりも抵抗が大きくなったことで流れる電流が1つの時より少なくなったから\n",
      "1694 　　　 ◯ 電熱線の抵抗が強かったため電流が流れにくかったから。\n",
      "1725 　　　 × 抵抗が大きい分電流が必要だから。\n",
      "1766 　　　 × 電気の低抗が大きくなったため、電流が流れにくくなったから\n",
      "1814 　　　 ◯ 抵抗が大きく電流が流れにくくあまり発熱しなかったから\n",
      "1847 　　　 × 低抗が大きくなり、電流が流れにくくなったため。\n",
      "1850 　　　 ◯ 抵抗が大きくなり電流が流れずらくなったから\n",
      "1874 　　　 × 抵抗が多くて、電流が流れにくかったから\n",
      "1915 　　　 ◯ 抵抗が2倍になり、電流が1/2倍になったから。\n",
      "1917 　　　 ◯ 抵抗も倍になるため、流れる電流の量はへるから。\n",
      "1958 　　　 × 2つの抵抗を直列につなぐと、うける電流は1つのときよりも弱くなってしまうから\n",
      "2000 　　　 ◯ 抵抗が大きくなる程、電流は小さくなるから\n",
      "誤採点検出回数： 55 1\n",
      "検出回数： 2\n",
      "==================================================\n",
      "16 誤採点 ◯ 電熱線が2つになることで低抗が大きくなるため流れる電流が少なくなるから\n",
      "36 　　　 × 抵抗が小さくなり、電流の大きさも小さくなったから。\n",
      "137 　　　 ◯ 抵抗が大きく、流れる電流が少なかったから。\n",
      "150 　　　 × 抵抗を多くつなぐともっと電流がながれにくくなり、水の上昇温度は下がること。\n",
      "169 　　　 × 電熱線が2つになったことで、電流が流れる大きさが小さくなり、その分電気低抗の大きさが大きくなったから。\n",
      "232 　　　 ◯ 直流でつなぐと抵抗が増え、電流が小さくなるため。\n",
      "241 　　　 × 電流の大きさを大きくすると抵抗は電流の2倍大きくなるから\n",
      "243 　　　 ◯ 抵抗の値が増えて、電流が通りづらくなったから\n",
      "259 　　　 × 抵抗が大きくなり電流はそのままだから\n",
      "260 　　　 × 8Vでやった場合2本だと抵抗がおおきすぎるから\n",
      "350 　　　 × 抵抗が小さくなり、電流が小さくなったから。\n",
      "351 　　　 × 抵抗の大きさが大きくなっても電流は一定だから。\n",
      "431 　　　 × 直列につなぐと抵抗が小さくなり電流も小さくなるから。\n",
      "432 　　　 ◯ 2つになったことによって抵抗が大きくなったので電流がよわまったから。\n",
      "438 　　　 ◯ 抵抗がふえたことにより、電流が流れにくくなったから。\n",
      "469 　　　 × 電流が大きけけば大きいほど、抵抗が大きいから。\n",
      "506 　　　 ◯ 電熱線が1つのときよりも、2つのときの方が抵抗が大きいので、電流が流れにくくなっていたから。\n",
      "542 　　　 ◯ 抵抗が大きくなったので、流れる電流が小さくなり、出る熱量がへったため。\n",
      "550 　　　 × 水の中だったため、電流が流れにくく、抵抗が実験の時より大きかったから、\n",
      "552 　　　 ◯ 抵抗が大きくなるので電流が流れずらくなるから。\n",
      "553 　　　 × 抵抗の大きさに対する電流の大きさが電熱線が1つの時よりも小さかったから。\n",
      "582 　　　 × 抵抗が分散されて、電流の大きさが小さくなった。\n",
      "596 　　　 × 一気に操作を行ったため、抵抗がすくなくなり、電流があまり流れなくなってしまったから。\n",
      "622 　　　 × 抵抗が2つになったことで電流が流れにくくなったから。\n",
      "646 　　　 ◯ 抵抗の大きさが大きくなり電流が流れにくくなったため。\n",
      "698 　　　 ◯ 抵抗が増えたことで、電流が通りにくくなったから。\n",
      "700 　　　 × 電流の大きさが大きくなり、抵抗の力が減って小さくなったから。\n",
      "727 　　　 ◯ 抵抗が大きくなり電流が流れづらくなってしまうから\n",
      "742 　　　 ◯ 電熱線が1つから2つにかわったとき、抵抗が強くなり、電流が弱くなるから。\n",
      "743 　　　 × 電流の大きさは変わらないのに対し、抵抗の大きさが大きくなったから\n",
      "746 　　　 ◯ 電熱線を2つにしたことにより抵抗も2倍になり電流が小さくなったから。\n",
      "837 　　　 ◯ 電熱線が増えたことで、抵抗が大きくなり、電流が流れにくくなったから。\n",
      "846 　　　 ◯ 抵抗の大きさが2倍になったので電流の大きさが1/2の大きさになってしまたから。\n",
      "853 　　　 ◯ 抵抗が大きくなり、電流が流れにくくなったため。\n",
      "857 　　　 × 電圧の大きさが小さくなり、電流が流れにくくなってしまったから。\n",
      "869 　　　 ◯ 抵抗が大きく、電流が流れにくくなったため。\n",
      "920 　　　 ◯ 電熱線を2つにしたことにより、2つ分の抵抗がかかり、1つの電熱線のときよりも、流れる電流が小さくなったから。\n",
      "923 　　　 ◯ 抵抗をふやしたことて電流が小さくなったから。\n",
      "928 　　　 ◯ 抵抗が大きすぎて電流が小さくなったから\n",
      "952 　　　 ◯ 電熱線を直列につないだので、回路全体の抵抗は、実験のときよりも大きくなるため、電流は実験のときよりも、流れなくなったから。\n",
      "978 　　　 ◯ 抵抗が大きくなったため、電流が電熱線が一つのときよりも流れにくくなったから。\n",
      "1001 　　　 × 直列回路したため電流は小さくなり、抵抗が大きくなったため上昇温度が低くなった。\n",
      "1003 　　　 ◯ 8.0Vに対する抵抗が大きくなってしまったため、電流が流れにくくなり、温度が上がりにくくなったから。\n",
      "1005 　　　 × 電圧が大きいと、抵抗が同じ大きさでも、電流が小さくなるから。\n",
      "1057 　　　 × 抵抗が生まれ電流があまりながれなかったから\n",
      "1080 　　　 × 抵抗が同じでも、電流が2倍に増えるので上昇する温度は、へってしまうから。\n",
      "1137 　　　 × 低抗が大きく電流が流れにくいから。\n",
      "1195 　　　 × 直列回路だと電流が流れる量が少ないので抵抗もおおきくなる\n",
      "1242 　　　 ◯ 抵抗が、電熱線1つのときより大きくなったので電流が流れにくくなったから。\n",
      "1272 　　　 ◯ 1つのときより抵抗が大きくなって熱量に使われる電流が小さくなったから。\n",
      "1296 　　　 ◯ 電熱線を２つ直列につないで抵抗がふえ、回路に流れる電流が小さくなったから。\n",
      "1311 　　　 ◯ 抵抗が大きいと、電流が流れにくくなるから。\n",
      "1320 　　　 × 電熱線が増えたことにより抵抗が増え必要な電流が2倍になったのに電熱線1つ分しかないから\n",
      "1372 　　　 ◯ 電熱線を増やしたことで、回路全体の抵抗が大きくなったため、電流が流れにくくなったから。\n",
      "1377 　　　 ◯ 電熱線が増えることで抵抗も強くなり、電流が流れにくくなるから、\n",
      "1382 　　　 × 水が抵抗となり、流れる電流の大きさが小さくなったから。\n",
      "1441 　　　 ◯ 抵抗が大きくなり、電流が流れにくくなったため。\n",
      "1452 　　　 ◯ 2つの伝熱線を使うと、抵抗が大きくなり、電流が流れずらくなるから\n",
      "1484 　　　 ◯ 電熱線を増やしたことで、抵抗が大きくなり、電流がよく流れなくなったから。\n",
      "1528 　　　 × 抵抗が高く、電流があまり流れなかったから。\n",
      "1553 　　　 × 抵抗がへって電流が増えたから\n",
      "1585 　　　 ◯ 抵抗が大きく、あまり電流が流れなかった\n",
      "1596 　　　 × 抵抗を増やしたら、電流が分散してしまうから。\n",
      "1635 　　　 ◯ 抵抗が1つ増えたので大きくなり、電流が流れにくい状態になったから。\n",
      "1643 　　　 ◯ 1つのときよりも、抵抗が大きくなり電流が流れにくくなったから\n",
      "1703 　　　 × 抵抗は1に比べて2つあることで大きくなって電流も流れているから、温度は、低くかった。\n",
      "1711 　　　 ◯ 抵抗が2倍になるので、オームの法則により、流れる電流の大きさが1/2になるから。\n",
      "1733 　　　 × 電熱線を増やしたことにより、抵抗が増え、電流がさえぎられる量も多くなるので水に伝わる熱量も減ったから。\n",
      "1774 　　　 ◯ 抵抗が大きくなり、電流が流れにくくなったため。\n",
      "1784 　　　 ◯ 抵抗は大きくなったが、電流は小さくなったから。\n",
      "1790 　　　 ◯ 電熱線を直列つなぎにすると回路全体の抵抗は大きくなり回路全体を流れる電流は電熱線1つのときと比べると小さくなってしまうから。\n",
      "1808 　　　 × 電熱線をふやすと、抵抗が大きくなり、つかう電流をふえるから。\n",
      "1812 　　　 ◯ 抵抗が大きくなり電流が低くなったから。\n",
      "1864 　　　 ◯ 抵抗を増やしたことで水に伝える電流の量が少なくなったと考えられるから。\n",
      "1865 　　　 × 抵抗が実験のときよりも大きくなったから。\n",
      "1866 　　　 ◯ 抵抗が大きいほど電流が流れにくいので水に熱が伝わりにくいため、\n",
      "1891 　　　 × 抵抗がおこり、電流が流れにくくなったから。\n",
      "1972 　　　 ◯ 抵抗の大きさが実験より、大きく、電流が伝わりづらかったから。\n",
      "誤採点検出回数： 78 1\n",
      "検出回数： 1\n",
      "==================================================\n",
      "26 　　　 ◯ 抵抗がふえたことにより電流が流れにくくなったため。\n",
      "30 　　　 × 低抗が1つに対して電熱線を2つにすると電流は少なくなるから。\n",
      "65 　　　 ◯ 直列回路において、全体の抵抗の大きさはそれぞれの抵抗の大きさの和になるので、電流の値は小さくなるから。\n",
      "71 　　　 ◯ 抵抗が大きくなって電流が流れにくくなったから\n",
      "89 　　　 ◯ 1つのときよりも抵抗が大きくなるため電流が少ししか流れなくなる\n",
      "101 　　　 ◯ 抵抗の合計が増え、流れる電流が少なくなったから。\n",
      "109 　　　 ◯ 抵抗がつよくて電流が流れづらかったため。\n",
      "111 　　　 ◯ 電熱線が1つのときより抵抗が大きく、電流が小さくなるから。\n",
      "115 　　　 ◯ 抵抗が大きくなり、電流がながれにくかった\n",
      "158 　　　 × 電熱線を2つにしたことで、抵抗が大きくなり、電流の大きさはかわっていないから。\n",
      "171 　　　 ◯ 電熱線が増えたことで抵抗が大きくなり電流の大きさが小さくなった。\n",
      "208 　　　 × 流れる電流は、同じだが抵抗が大きくなるため、発熱量が少なくなるから。\n",
      "213 　　　 × 回路の抵抗が上がり、電流が下がったから。\n",
      "223 　　　 × 抵抗が少なく、電流があまり流れないから\n",
      "261 　　　 ◯ 電熱線を2つにすると抵抗が大きくなり、電流が流れにくくなるから\n",
      "274 　　　 ◯ 抵抗がおおきく電流がながれにくかったから。\n",
      "279 　　　 × 電圧を下げたことにより、抵抗は変わらないが電流が小さくなったから。\n",
      "284 　　　 ◯ 電熱線の抵抗の大きさが大きくなり、電流が流れにくくなったため。\n",
      "295 　　　 × 電流が小さく抵抗も小さかったから。\n",
      "314 　　　 ◯ 抵抗が２つになり、それぞれに流れる電流が小さくなったから、\n",
      "344 　　　 × 全体の抵抗が大きくなったのに電熱線を流れる電流の大きさが変わらないから。\n",
      "440 　　　 ◯ 抵抗の大きさが同じ電熱線を直列につなぐと抵抗も比例して大きくなり、電流が流れにくくなるから。\n",
      "474 　　　 × 電流が少なく抵抗が小さかったから\n",
      "491 　　　 ◯ 抵抗が大きくなって電流が流れにくくなったから\n",
      "501 　　　 ◯ 抵抗が大きくなったため、電流が流れにくくなったから。\n",
      "517 　　　 ◯ 2つ入っている分、抵抗が大きくなり電流が流れにくくなったから。\n",
      "561 　　　 ◯ 抵抗が大きく、電流が流れずらかったから\n",
      "591 　　　 × 電流を流す時の抵抗が1つの時より大きくなったから\n",
      "598 　　　 × 電熱線の抵抗が減り、電流がスムーズに流れるようになったから\n",
      "608 　　　 × 電熱線を2本にしてしまったことによって電流が小さくなり抵抗も小さくなってしまったから。\n",
      "662 　　　 ◯ 抵抗が大きくなり、電流が流れにくくなったため。\n",
      "674 　　　 × 電熱線が2つだと電流が大きくなり、抵抗が大きいから。\n",
      "676 　　　 × 抵抗がおうきすぎて電流があまり流れなかった。\n",
      "681 　　　 ◯ 電熱線を２つ入れると抵抗が強くなり、電流が流れにくくなったから。\n",
      "688 　　　 ◯ 抵抗力が大きくうまく電流が流れなかった。\n",
      "722 　　　 ◯ 抵抗が増えて電流の大きさを小さくしたから。\n",
      "771 　　　 ◯ 抵抗が２倍になってしまうため、電流が1/2しか流れなくなるから。\n",
      "775 　　　 × 抵抗の大きさが同じでも電流の大きさは小さいから。\n",
      "796 　　　 × 抵抗の大きさが同じだと合計で10.0Ωになり、それを直列でつないでいるため、電流が1.60Aのままだから。\n",
      "824 　　　 ◯ 抵抗が大きくなったため、電流が通りにくくなったから\n",
      "862 　　　 × 電熱線の抵抗が下がり、電流がたくさん流れないから。\n",
      "875 　　　 × 抵抗が小さくなり、電流が流れやすくなったから。\n",
      "902 　　　 ◯ 直列だと抵抗が大きくなり、電流は流れにくくなるから。\n",
      "906 　　　 × 電熱線の抵抗により、電流が少なくなったため温度が少なくなった。\n",
      "959 　　　 ◯ 抵抗が大きくなったので水に伝わる電流が少なくなったから。\n",
      "965 　　　 ◯ 電流を流すときの抵抗の大きさが大きくなったので電流が流れにくくなってしまった。\n",
      "973 　　　 ◯ 電熱線が２つになると抵抗も2倍になってしまうため同じ電流では弱まってしまうから。\n",
      "994 　　　 × 抵抗が弱くなり、電流も流れにくくなったから。\n",
      "1007 　　　 ◯ 電熱線を直列につなぐことで抵抗が大きくなり電流が流れにくくなった。\n",
      "1011 　　　 × 装置全体の抵抗が上がったので、電流がさがるから\n",
      "1021 　　　 ◯ 2つにしたことにより抵抗が大きくなり電流が小さくなってしまったから\n",
      "1025 　　　 ◯ 電熱線が2つになったことで抵抗が大きくなり電流が流れにくくなったから\n",
      "1027 　　　 ◯ 2つの電熱線を直列につないで電流を流すと、2つの電熱線の抵抗の和が回路全体の抵抗になり、流れる電流が小さくなるから。\n",
      "1073 　　　 ◯ 電熱線を増やしたため抵抗が大きくなり電流が流れづらくなったから。\n",
      "1088 　　　 ◯ 2つの電熱線にしたことにより、抵抗が大きくなり電流が流れにくくなったから。\n",
      "1092 　　　 ◯ 抵抗が大きくなり電流が流れにくくなるから\n",
      "1097 　　　 ◯ 電熱線の抵抗が大きくなり電流が流れにくくなるから。\n",
      "1113 　　　 ◯ 電熱線が1つ増えると、その分、抵抗も大きくなり電流が流れにくくなったから。\n",
      "1124 　　　 ◯ 抵抗をふやしたことで、電流が流れにくくなったから。\n",
      "1151 　　　 ◯ 電熱線が1つのときよりも、抵抗が大きくなり電流が流れにくくなったから。\n",
      "1162 　　　 ◯ 2つの電熱線により抵抗が大きくなり電流が流れにくくなったため。\n",
      "1164 　　　 ◯ 抵抗が大きく、電流があまり流れなかったから。\n",
      "1172 　　　 ◯ 抵抗が同じものをつつつかったことで抵抗が2倍になり電流の流れた量が減ったから。\n",
      "1196 　　　 × 電熱線の数を増やすと抵抗の大きさよりも電流の大きさのほうが大きくなったから\n",
      "1211 　　　 ◯ 抵抗が大きくなり、電流の流れる量がへったから。\n",
      "1240 　　　 ◯ 抵抗の大きさが大きく電流のつたわる量が小さかったため\n",
      "1244 　　　 ◯ 抵抗が2倍になったので、電流が流れにくくなったから。\n",
      "1318 　　　 × 抵抗が増えたのに対し電圧をおとしたけっか、電流が下がったため。\n",
      "1352 　　　 ◯ 電熱線をふやしたことで抵抗が大きくなり、電流が流れにくくなったから。\n",
      "1358 　　　 ◯ 電熱線がふえ抵抗が大きくなり電流がながれにくくなったから\n",
      "1368 　　　 × 抵抗が低く、電流が流れにくいから。\n",
      "1370 　　　 × 抵抗がすくなく電流がながれやすいから。\n",
      "1378 　　　 ◯ 抵抗が大きくなったため電流が流れずらくなったから。\n",
      "1390 　　　 ◯ 抵抗が１つときよりも倍になってしまい、電流が流れづらくなってしまうから\n",
      "1392 　　　 ◯ 電熱線が1つのときよりも、抵抗が大きくなって、電流が流れにくくなったから。\n",
      "1399 　　　 × 電流がつよくなったため抵抗がよわくなった\n",
      "1413 　　　 × 電気抵抗により、2つの電熱線に流れる電流が変わるから。\n",
      "1423 　　　 ◯ 抵抗が増えたから電流が流れづらく、温度が上がらなかった、\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1479 　　　 ◯ 2つの電熱線を直列につないだことにより、抵抗が大きくなり電流が流れにくくなったから。\n",
      "1504 　　　 × 直例回路はどこも電流が同じで、抵抗はそれぞれの電熱線の抵抗の和になるため、電圧が小さくなるから。\n",
      "1514 　　　 × 電熱線の抵抗が上がり、電流は直列つなぎの方が並列つなぎよりよわいから。\n",
      "1530 　　　 ◯ 2つの電熱線を直列につなぐと、抵抗が大きくなり電流があまり通らないから。\n",
      "1542 　　　 × 抵抗が低ければ、電流も低くなるから。\n",
      "1571 　　　 ◯ 2つの電熱線を直列につないだため、抵抗が大きくなり、電流が通りにくくなったから。\n",
      "1616 　　　 × 低抗が大きくなり電流が小さくなったから。\n",
      "1630 　　　 ◯ 抵抗を増やした事によって、電流の流れる量が減ったから。\n",
      "1656 　　　 ◯ 電熱線が増えたので、抵抗が大きくなり、電流が伝わる大きさが小さくなった。\n",
      "1673 　　　 ◯ 2つの電熱線を直列につないでいるため、抵抗が大きく、電流が小さくなったから。\n",
      "1706 　　　 × 直列回路の抵抗はそれぞれが全体と等しくなり電流は2つの電熱線の和が全体と等しくなるため\n",
      "1749 　　　 ◯ 直列につなげた回路なので抵抗が大きくなり、電流の大きさが小さくなったため。\n",
      "1789 　　　 × 電熱線をふやすと抵抗がおきて電流がながれないから\n",
      "1801 　　　 ◯ 抵抗が大きいのに対して電流が小さかったから\n",
      "1825 　　　 ◯ 抵抗が大きくなり、電流が流れにくくなったため。\n",
      "1855 　　　 ◯ 抵抗が大きくなると、電流の大きさも減り、上昇温度が低くなるから。\n",
      "1868 　　　 × 低抗の大きさが強く、電流の大きさが弱いため。\n",
      "1878 　　　 ◯ 抵抗の値が大きくなり、流れる電流の値が少さくなったから。\n",
      "1903 　　　 ◯ 抵抗が大きくなると電流は小さくなるので、水に伝わる温度が低くなるから。\n",
      "1992 　　　 ◯ 発熱線が増えたことにより、抵抗が大きくなって電流が流れにくくなったから。\n",
      "2002 　　　 × 抵抗して電流が流れにくくなったから。\n",
      "誤採点検出回数： 99 0\n",
      "検出回数： 0\n",
      "==================================================\n",
      "0 　　　 × 抵抗が大きくなり、電圧が下がったため\n",
      "9 　　　 ◯ 抵抗が大きくなったため、電流が流れにくくなったから。\n",
      "37 　　　 × ていこうが2つのときのほうが電流が流れにくいから。\n",
      "66 　　　 ◯ 抵抗が大きくなり、電流が小さくなるから。\n",
      "72 　　　 × 電熱線を2本にした時の方が、電流が流れやすくなり、抵抗が弱まるため。\n",
      "76 　　　 ◯ 直列につなぐと抵抗が大きくなるため電流が流れにくくなったから。\n",
      "83 　　　 ◯ 抵抗が大きくなったことにより、電流の合計が小さくなったから。\n",
      "85 　　　 × 電熱熱がもう1つついたことにより抵抗の大きさが大きくなったから\n",
      "113 　　　 ◯ 抵抗が大きく、電流が流れにくかったため\n",
      "122 　　　 ◯ 抵抗が大きくなり電流が流れにくくなったから\n",
      "125 　　　 ◯ 電熱線を2つにしたので抵抗が大きくなったため、その分、電流が小さくなったから\n",
      "133 　　　 × 抵抗によって電流がよわくなってしまったから\n",
      "170 　　　 ◯ 電熱線がふえたので、その分抵抗も大きくなり電流の流れが小さくなったから。\n",
      "173 　　　 ◯ 2つの電熱線を直列につないだことで回路の抵抗が大きくなり、流れる電流の大きさが小さくなったから。\n",
      "174 　　　 ◯ 電熱線が実験よりも増えたので抵抗も増えて電熱線1つに流れる電流が小さくなったから。\n",
      "177 　　　 ◯ 抵抗の大きさが増え、流れる電流が少なくなったから。\n",
      "185 　　　 × 電流が全てに流れるため、1つに流れてる時よりも、電流が小さく、抵抗が大きいから。\n",
      "209 　　　 ◯ 抵抗が大きくなったため、電流が流れにくくなったから。\n",
      "214 　　　 × 電熱線が2つになったため、電流を流すときの抵抗が2倍になったから。\n",
      "218 　　　 ◯ 電熱線を直列につなぐと、抵抗が大きくなって電流が流れにくくなり、発熱量が少なくなるから。\n",
      "228 　　　 × 2つの電熱線を直列につないだため抵抗し、電流が小さくなった\n",
      "237 　　　 ◯ 抵抗の量が大きくなり、電流の大きさが小さくなるから。\n",
      "246 　　　 × 電気抵抗が大きかったため、電熱線ではなくほかのところで電流が流れてしまったから。\n",
      "252 　　　 ◯ 抵抗が大きくなり電流が流れにくくなるため。\n",
      "255 　　　 ◯ 抵抗がふえて、電流が流れづらくなってしまったから。\n",
      "258 　　　 ◯ 電熱線を増やすと、抵抗が大きくなり、電流が流れにくくなるから。\n",
      "263 　　　 × 電流を大きくすると、抵抗も大きくなるのでその間に熱がにげていってしまうから。\n",
      "271 　　　 ◯ 抵抗が大きくなって電流が流れにくくなったから\n",
      "285 　　　 × 2つの電熱線を直列につなぐと、抵抗は足し算されるので、電流が流れにくくなったから。\n",
      "300 　　　 ◯ 抵抗をふやしたことにより、電流が流れにくくなったから。\n",
      "316 　　　 ◯ 抵抗が大きくなり電流が流れにくくなったから\n",
      "329 　　　 × 抵抗が小さく、流れている電流も、小さかった。\n",
      "346 　　　 ◯ 抵抗が大きくなり電流が流れにくくなったから\n",
      "353 　　　 ◯ 一つの場合よりも抵抗が大きいため電流が流れにくいため発熱量も低くなる\n",
      "370 　　　 ◯ 抵抗が大きくなり電流がうまく流れないため\n",
      "376 　　　 ◯ 電熱線が1つになると、抵抗が大きくなるため、電流が流れにくくなり、熱エネルギーになりにくくなるから。\n",
      "396 　　　 ◯ 抵抗が大きくなり電流が流れにくくなったから\n",
      "423 　　　 × 電流の大きさが半分になり、抵抗も半分になり、小さくなったから。\n",
      "430 　　　 ◯ 電熱線が2つにふえて抵抗の大きさが強くなって電流が流れづらくなったから。\n",
      "433 　　　 × 直列なので抵抗に流れる電流が少くなってしまったから\n",
      "441 　　　 ◯ 電熱線1コの時よりも抵抗が強くなり電流の量も減ってしまったから。\n",
      "443 　　　 ◯ 抵抗の大きさが大きくなり、電流が流れにくくなったため。\n",
      "475 　　　 ◯ 抵抗が大きかったため電流の大きさが小さかったから\n",
      "480 　　　 ◯ 抵抗が大きくなったため、電流が流れにくくなったから。\n",
      "482 　　　 ◯ 電熱を増すと抵抗が大きくなるため、電流が小さくなった。\n",
      "489 　　　 ◯ 抵抗が大きくなり電流が流れづらくなったため。\n",
      "502 　　　 ◯ 抵抗が大きくなることで電流が小さくなり、電力も小さくなることで熱量も小さくなるから。\n",
      "510 　　　 ◯ 電熱線を直列につないだことで抵抗が大きくなり、電流が小さくなったので電力も小さくなったから。\n",
      "511 　　　 × 電流が小さく、抵抗が大きかったから。\n",
      "512 　　　 × でんりゅうがていこうによっておせえられたから。\n",
      "519 　　　 × 抵抗が同じでも、流れる電流が二つあるため１つのときよりも水の上昇温度は低いから。\n",
      "523 　　　 × 電熱線が2つになった分、電流が半分に分かれるようになり、それにともなって、抵抗も小さくなるから。\n",
      "525 　　　 ◯ 電熱線が多いほど抵抗が大きく電流が流れにくくなり、水をあたためにくくなるから、\n",
      "529 　　　 ◯ 抵抗が大きくなり、電流が流れにくかったから。\n",
      "531 　　　 × 1つの電熱線に流れる電流が少なくなり抵抗が小さくなったから。\n",
      "543 　　　 ◯ 電熱線を直列につなぐと、抵抗の大きさが大きくなり、電流がながれにくくなったから。\n",
      "548 　　　 × 直列回路は、どこでも電流が等しく、全体の抵抗は、電熱線2つの抵抗の合計になるため、全体の抵抗が大きくなるから。\n",
      "576 　　　 ◯ 電熱線をふやしたことによって抵抗がさらに増え電流が弱まってしまったため\n",
      "580 　　　 ◯ 抵抗が大きくなったため、電流が流れにくくなったから。\n",
      "588 　　　 ◯ 抵抗が大きくなり、電流が流れにくくなったから\n",
      "613 　　　 ◯ 抵抗が大きくなり、電流が流れにくくなるから\n",
      "625 　　　 × 抵抗が大きくなり電流が実験に使われている道具に流れてしまったため。\n",
      "626 　　　 × 抵抗が少なくなり電流が流れやすくなったから\n",
      "637 　　　 ◯ 直列につなぐと抵抗が1つのときよりも大きくなり、電圧も同じ大きさしかかからないため、流れる電流が小さくなるから。\n",
      "639 　　　 × 低抗が大きくなり電流があまりあがらなかった。\n",
      "652 　　　 ◯ 2つの電熱線を直列につないでしまうと、抵抗が大きくなってしまい、電流が流れにくくなってしまうから。\n",
      "653 　　　 ◯ 抵抗が大きく電流の強さが弱くなったため。\n",
      "654 　　　 ◯ 全体の抵抗が実験のときの抵抗よりも、大きくなり、電流が流れにくくなったから。\n",
      "660 　　　 ◯ 電圧の大きさが同じでも、抵抗の大きさは大きくなるので、電流の大きさが小さくなったため。\n",
      "664 　　　 ◯ 抵抗器を直列につなぐと抵抗が大きくなり、電流が流れにくくなったから。\n",
      "669 　　　 ◯ 抵抗がおおきくなり、電流がながれにくくなるから。\n",
      "697 　　　 ◯ 抵抗が大きく、電流が小さかったから。\n",
      "701 　　　 ◯ 電熱線の発熱量は時間×電力であるから2つの場合、1つの場合よりも抵抗が大きく、電流がながれにくくなるため、電力が少なくなった。\n",
      "716 　　　 ◯ 直列回路では抵抗が大きく、電流が流れにくいから。\n",
      "748 　　　 ◯ 抵抗が大きかったので、電流が流れずらかったから。\n",
      "762 　　　 ◯ 抵抗が1つのときよりも大きくなり電流が小くなってしまうから\n",
      "772 　　　 × 抵抗によって電流が流れにくくなるから。\n",
      "776 　　　 × 抵抗によって、電流が流れにくくなった。\n",
      "783 　　　 ◯ 抵抗が２倍になったため、電流の大きさが半分になったから\n",
      "786 　　　 ◯ 直列に2つつないだ抵抗の大きさは1つのときより大きいので電流が小さくなったため。\n",
      "787 　　　 ◯ 電熱線が2つになったことで、抵抗が大きくなり、電流が流れにくくなったから。\n",
      "790 　　　 ◯ 抵抗が大きくなり電流が小さくなるため。\n",
      "804 　　　 ◯ 抵抗が大きくなり電流が流れにくくなったから。\n",
      "806 　　　 × 電熱線が1つの時よりも2つの時の方が流れる電流が抵抗し、水の上昇温度が低くなる。\n",
      "825 　　　 × 電熱線を1つ増したことで、低抗が小さくなり、電流の流れがよくなったから。\n",
      "838 　　　 × 電流が大きくなって抵抗が小さくなったから。\n",
      "839 　　　 ◯ 抵抗が大きくなり、電流が小さくなって、熱量が減ったから。\n",
      "843 　　　 × 電熱線を2つつかうと電流の大きさが減り抵抗が増えてしまうから\n",
      "861 誤採点 ◯ 電流が小さく抵抗が大きかったから\n",
      "872 　　　 ◯ 抵抗が高いと電流の大きさが小さくなるので電力量が減るから。\n",
      "883 　　　 ◯ 2つの電熱線を直列につないだことにより、抵抗が大きくなり、電流が小さくなったから。\n",
      "890 　　　 ◯ 抵抗が大きく、電流が小さかったため、熱が伝わりにくかったから。\n",
      "894 　　　 ◯ 抵抗が大きくなり電流が流れにくくなったから\n",
      "908 　　　 ◯ 抵抗の大きさが大きくなったため、電流の大きさも小さくなったから。\n",
      "935 　　　 × 電流の量が低く、抵抗の量が増えたから\n",
      "944 　　　 × 直列回路は並列回路よりも電流は同じ大きさだが、抵抗は直列回路の方が大きくなるから\n",
      "949 　　　 ◯ 電熱線が2つになったので、抵抗が大きくなり電流が小さくなってしまったから。\n",
      "967 　　　 ◯ 抵抗が大きいため、電流が流れにくかったから。\n",
      "977 　　　 ◯ 抵抗が大きくなり電流が流れにくくなったから。\n",
      "991 　　　 ◯ 抵抗が大きくなったので、電流が流れずらかったから\n",
      "998 　　　 ◯ 電気を流す抵抗が電熱線1つのときよりも大きくなり、電流が弱くなったから\n",
      "1016 　　　 × 電流の大きさが大きくなれば、抵抗の大きさも大きくなるから\n",
      "1018 　　　 ◯ 抵抗が大きく、電流が流れにくくなったから\n",
      "1049 　　　 ◯ 電熱線が1つのときよりも抵抗が大きく、電流が流れにくいから。\n",
      "1056 　　　 × 電流が大きくなるとそれにともなって抵抗の大きさも大きくなるから\n",
      "1068 　　　 × 電流の大きさが2つにしたことで小さくなり、それにともない抵抗も小さくなったから。\n",
      "1076 　　　 ◯ 電熱線を直列につないだことによって抵抗の大きさが大きくなり、電流が流れにくくなったから。\n",
      "1081 　　　 ◯ 回路を直列につなぐと、抵抗の値が大きくなり、電流の大きさが、小さくなるため。\n",
      "1095 　　　 ◯ 抵抗が大きくなったため、電流が流れにくくなったから\n",
      "1117 　　　 × 抵抗の大きさが大きくなって、電流の大きさは変わらなかったから\n",
      "1121 　　　 ◯ 電熱線が2つになると、抵抗が大きくなって、電流を流しにくくするから。\n",
      "1129 　　　 ◯ 電熱線を2つにしたので、抵抗が大きくなり、電流が流れにくくなったから。\n",
      "1136 　　　 ◯ 電熱線を2つにしたことで、抵抗の力が大きくなり、電流が流れにくくなったから\n",
      "1138 　　　 × 電流が小さく、抵抗が大きかったから。\n",
      "1152 　　　 × 流れる電流が大きくなって抵抗が小さくなったから。\n",
      "1154 　　　 × 電圧を抵抗するものが増え、電流が水に伝わりにくなったから。\n",
      "1155 　　　 ◯ 直列回路では抵抗の大きさが1つの電熱線よりも2つの電熱線のときの方が大きいので、電流が流れにくくなったから、\n",
      "1163 　　　 × 電流が弱く抵抗が少なかったから。\n",
      "1168 　　　 ◯ 抵抗が大きく、電流が伝わりづらかったから。\n",
      "1170 　　　 ◯ 抵抗を直列につなぐと、1つのときより抵抗が大きくなり、電流が流れにくくなるので、電力も小さくなるから。\n",
      "1181 　　　 × 電熱線が増えたので、電流を流すと抵抗が大きくなるため、水の温度は上がりずらくなるから。\n",
      "1192 　　　 ◯ 抵抗が大きくなったため、電流が流れにくくなったから。\n",
      "1204 　　　 × 電流が流れにくく、抵抗が大きくため\n",
      "1214 　　　 × 電熱線の低抗が強くなり、電流が流れにくくなってしまうから。\n",
      "1218 　　　 × 抵抗が小さくなり、電流が流れにくくなったから。\n",
      "1219 　　　 × 抵抗によって電流の強さが低くなったから。\n",
      "1239 　　　 ◯ 抵抗が大きくなると電流の大きさも減り、上昇温度が下がるから。\n",
      "1257 　　　 ◯ 電熱線を直列につないだことで、抵抗が増加し、電流の大きさが小さくなったため。\n",
      "1268 　　　 ◯ 直列つなぎの時、回路全体の抵抗の大きさは抵抗の和になるので、抵抗が大きくなり、電流が流れにくくなるから。\n",
      "1275 　　　 ◯ 電熱線の抵抗が大きくなり電流が通りにくくなったから。\n",
      "1288 　　　 ◯ 2つの電熱線を直列につなぐことで、抵抗が大きくなり、電流が流れにくくなるから。\n",
      "1289 　　　 ◯ 直列回路にしていることで回路の抵抗は2つの電熱線の抵抗の和となり、電熱線が一つのときよりも流れる電流が小さくなるから。\n",
      "1301 　　　 × 抵抗がはたらいて電流をあまりながさなかったため\n",
      "1312 　　　 ◯ 抵抗が強く、電流が流れにくいから。\n",
      "1316 　　　 × 抵抗は直列つなぎで電流を流すと小さくなるから。\n",
      "1319 　　　 ◯ 抵抗が大きくなってしまい電流が下がってしまったから。\n",
      "1330 　　　 ◯ 抵抗が大きく、電流が流れにくかったため。\n",
      "1334 　　　 ◯ 抵抗が増えたため電流があまり流れなかった。\n",
      "1345 　　　 × 電流は抵抗が非常に強いため。\n",
      "1347 　　　 ◯ 抵抗の大きさが大きくなったことにより、電流が流れにくくなったため\n",
      "1360 　　　 ◯ 抵抗が大きくなり、電流が流れにくくなるから\n",
      "1394 　　　 ◯ 電熱線が１つのときよりも抵抗が大きくて、電流が流れにくいから\n",
      "1395 　　　 ◯ 実験のときと比べ、抵抗が2倍になり、回路内を流れる電流が半分になるから。\n",
      "1401 　　　 ◯ 抵抗が増え、電流の大きさが小さくなってしまうから。\n",
      "1408 　　　 ◯ 電熱線が2つになったため、抵抗が大きくなり、電流が小さくなったから\n",
      "1428 　　　 ◯ 抵抗の大きさが大きくなると電流は流れにくくなるから。\n",
      "1447 　　　 × 電流\n",
      "1465 　　　 × 抵抗に多くの電流が流れ、電熱線に流れる電流が減ったから。\n",
      "1468 　　　 ◯ 抵抗が2倍になり、電流が流れにくくなったから。\n",
      "1478 　　　 ◯ 電熱線が2個だったから、抵抗の大きさが大きくなり、電流が小さくなったから。\n",
      "1485 　　　 × 電流に対する抵抗が1つのときよりも大きくなったから、\n",
      "1495 　　　 ◯ 抵抗が大きくなったことで、電流の大きさが、小さくなったから。\n",
      "1498 　　　 ◯ 抵抗が大きくなり水につたわる電流が小さくなったから。\n",
      "1511 　　　 ◯ 電熱線を2つつないだため抵抗が大きくなったから電流が流れにくくなった\n",
      "1548 　　　 ◯ 抵抗が大きいため、電流が流れにくくなるから。\n",
      "1562 　　　 ◯ 抵抗が大きいから電流を通しにくいため\n",
      "1566 　　　 ◯ 抵抗が大きくなり、電流の大きさが半分になったから\n",
      "1574 　　　 ◯ 抵抗が大きくなったので電流があまり流れなくなった\n",
      "1578 　　　 ◯ 抵抗する力が大きくなり、電流が流れにくくなったから。\n",
      "1589 　　　 × 電流を流したときの抵抗が大きくなったから。\n",
      "1599 　　　 ◯ 2つの抵抗を直列回路にすると、抵抗が1つのときよりも大きくなり、電流が流れにくくなるから。\n",
      "1601 　　　 × 電流は同じ大きさで流れるが、抵抗の大きさは2倍になったため。\n",
      "1606 　　　 ◯ 抵抗がおおきくなり電流が流れにくくなったため。\n",
      "1619 　　　 ◯ 抵抗の力が大きく、電流が伝わりにくくなってしまった。\n",
      "1638 　　　 × 抵抗が2つになったので、それぞれの抵抗に電流が流れるよう2つに分かれてしまったから。\n",
      "1658 　　　 ◯ 2つの電熱線で抵抗が強くなり電流が流れにくくなった\n",
      "1664 　　　 × 抵抗が小さく、電流が流れにくくなったため。\n",
      "1666 　　　 ◯ 2つの電熱線により抵抗が大きくなり電流がながれにくくなった\n",
      "1670 　　　 × 電流が大きくなるほど、低抗は小さくなるから。\n",
      "1676 　　　 × 抵抗が2つなので、電流が1つのときよりも流れにくくなったから。\n",
      "1682 誤採点 ◯ 直列につなぐと全体の低抗の大きさが大きくなってしまい、電流の大きさが小さくなってしまうから。\n",
      "1691 　　　 × 電流の大きさが大きかったため、抵抗が小さかったから。\n",
      "1699 　　　 × 電流の抵抗が1つの時よりも二つの時の方が大きいいから。\n",
      "1720 　　　 ◯ 電熱線が1つのときと比べて抵抗が2倍になったので電流が流れにくくなったから\n",
      "1722 　　　 × 2つの抵抗を使ったことで、電流は1/2になった。\n",
      "1731 　　　 ◯ 電流線がふえて、抵抗が大きくなったため電流が流れにくくなったから。\n",
      "1740 　　　 ◯ 電熱線が2つあることにより抵抗が大きくなり、電流が流れにくくなったから。\n",
      "1742 　　　 ◯ 2つの電熱線をつなげると抵抗が大きいため電流の大きさが小さくなるため上昇温度が低かった。\n",
      "1746 　　　 ◯ 直列つなぎは抵抗が大きくなるため、電流が流れにくくなるから。\n",
      "1747 　　　 ◯ 直列につないだため抵抗が大きくなり、回路を流れる電流の大きさが小さくなったから。\n",
      "1769 　　　 ◯ 抵抗が大きくなって電流が流れにくくなっているからです。\n",
      "1773 　　　 ◯ ２つの電熱線を使うと、その分抵抗も大きくなるので、電流の大きさが小さくなるから。\n",
      "1777 　　　 ◯ 直列回路は抵抗の大きさが、全体の抵抗の和になるため、抵抗が大きくなり、電流の大きさが小さくなったため。\n",
      "1779 　　　 ◯ 直列つなぎだと全体の抵抗が大きくなり、電流が流れにくくなるから。\n",
      "1781 　　　 ◯ 電熱線が1つのときよりも、2つのときの方が抵抗が大きくなり、電流が流れにくくなったから。\n",
      "1783 　　　 × 抵抗がすくないため電流が流れる。\n",
      "1809 　　　 ◯ 抵抗の大きさが大きくなると、電流の大きさが小さくなり、電流の大きさが小さくなると、熱量の大きさも小さくなるから。\n",
      "1820 　　　 ◯ 抵抗が大きくて、電流が流れにくかったから。\n",
      "1835 　　　 ◯ 電熱線を直列につなぐと、抵抗が大きくなり、電流が流れにくくなるから。\n",
      "1884 　　　 ◯ 抵抗が大きくなって電流が小さくなったから。\n",
      "1899 　　　 × 直列でつないでいるため抵抗はあまりないけど、電流の流れるきょりがながくなって、流れているうちに少し外に熱がでてしまうから\n",
      "1908 　　　 × 電熱線が増えると電流の大きさは変わらず抵抗の大きさが大きくなるから。\n",
      "1911 　　　 ◯ 一つのときより抵抗が大きく電流が流れづらいから\n",
      "1914 　　　 × 電熱線を2本に増やしたことにより、電流にかかる抵抗の割合が増えたため。\n",
      "1919 　　　 × 電流の大きさが2つの抵抗になったため1つより弱くなったため。\n",
      "1920 　　　 ◯ 直列でつなぐと抵抗がもとより大きくなり、電流が流れにくくなるため。\n",
      "1929 　　　 ◯ 抵抗が大きく電流が流れにくかったから、\n",
      "1932 　　　 × 2つの電熱線を直列につないだため、電流は変わらず、抵抗が図1より大きくなったから。\n",
      "1933 　　　 ◯ 抵抗が倍になるため電流が流れにくいから。\n",
      "1938 　　　 ◯ 2つの電熱線を使うと抵抗が大きくなり、電流が流れにくくなるから。\n",
      "1943 　　　 ◯ 抵抗は大きくなり、電流が流れにくくなってしまうから。\n",
      "1944 　　　 × 電流の流れが小さく、抵抗が大きいから。\n",
      "1947 　　　 ◯ 2つになったことで抵抗が増え電流が流れにくくなったから、\n",
      "1959 　　　 ◯ 電熱線を2つにすると抵抗が大きくなり、流れる電流が少さくなるから\n",
      "1967 　　　 ◯ 電熱線が1つのときよりも2つのほうが抵抗が大きく電流がながれにくくなっているから\n",
      "1989 　　　 ◯ 電熱線が2つになり、抵抗が大きくなり、電流が小さくなったから\n",
      "2003 　　　 × 低抗が倍になり、電流が流れにくくなったから。\n",
      "2013 　　　 × 抵抗の大きさが同じで電流の大きさが変わらないので、電熱線が増えると空気中に逃げてしまう熱が増えるため\n",
      "2015 　　　 ◯ 抵抗が大きくなったことで、電流が流れにくくなったから。\n",
      "誤採点検出回数： 209 2\n",
      "[47, 40, 28, 26, 24, 22, 25, 28, 29, 40, 51, 55, 78, 99, 209]\n",
      "[5, 11, 1, 3, 2, 1, 2, 0, 1, 2, 2, 1, 1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "miss_count = np.zeros(len(student_list), dtype=np.int64)\n",
    "for change_id in range(len(change_seeds)):\n",
    "    count = 0\n",
    "    for id, label in enumerate(all_ans_label[change_id]):\n",
    "         if label != new_student_answer_list[change_id][id]: #間違っていたら\n",
    "                miss_count[text_pos_memo[change_id][id]] += 1\n",
    "    \n",
    "siteki_num = []\n",
    "gosaiten_num = []\n",
    "\n",
    "out_list = []\n",
    "out_index = []\n",
    "\n",
    "for i in range(len(chaneg_seeds)):  \n",
    "    count = 0\n",
    "    all_count = 0\n",
    "    i = len(chaneg_seeds)-i\n",
    "    \n",
    "    print(\"検出回数：\", i)\n",
    "    print(\"==================================================\")\n",
    "    for id, num in enumerate(miss_count):\n",
    "        if num > i and num <= i + 1:\n",
    "            all_count += 1\n",
    "            if int(new_student_answer_list[0][id]) == 0:\n",
    "                label = \"◯\"\n",
    "            else:\n",
    "                label = \"×\"\n",
    "           \n",
    "            if change_flag ==1:\n",
    "                if student_answer_list[id] != check_student_answer_list[id]:\n",
    "                    \n",
    "                    text = \"誤採点\"\n",
    "                    count += 1\n",
    "                else:\n",
    "                    text = \"　　　\"\n",
    "                \n",
    "                print(id, text, label, new_student_list[0][id])\n",
    "            else:\n",
    "                print(id, label, new_student_list[0][id])\n",
    "                \n",
    "            out = text + \",\"+label+\",\"+new_student_list[0][id]+\"\\n\"\n",
    "            out_list.append(out)\n",
    "            out_index.append(str(index[id])+\"\\n\")\n",
    "            \n",
    "    print(\"誤採点検出回数：\", all_count, count)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    siteki_num.append(all_count)\n",
    "    gosaiten_num.append(count)\n",
    "    \n",
    "pos = 1\n",
    "print()\n",
    "for siteki, wrong in zip(siteki_num, gosaiten_num):\n",
    "    print(pos, \":\", sikteki, wrong)\n",
    "    pos+=1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
